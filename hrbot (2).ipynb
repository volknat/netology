{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hrbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id_UT3HX-wBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !export IMAGE_FAMILY=\"pytorch-latest-gpu\"\n",
        "# !export ZONE=\"us-west1-b\"\n",
        "# !export INSTANCE_NAME=\"pytorch-colab-backend\"\n",
        "# !gcloud compute ssh --zone ZONE INSTANCE_NAME -- -L 8888:localhost:8888"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFXa8SHrXzan",
        "colab_type": "code",
        "outputId": "922d67a8-d9ee-4e35-ad94-488014a6d7c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "!pip install transformers==2.5.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.5.0 in /usr/local/lib/python3.6/dist-packages (2.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (0.0.38)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (1.12.23)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.5.0 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (0.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (1.18.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (0.1.85)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.0) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.0) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.0) (1.12.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.0) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.0) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.0) (1.15.23)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.0) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.0) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.0) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers==2.5.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers==2.5.0) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOF2qFLicBj9",
        "colab_type": "code",
        "outputId": "23f9749b-a7ff-452e-fa04-205112421645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import BertModel, AdamW, BertConfig, BertTokenizer, Model2Model, PreTrainedEncoderDecoder, BertPreTrainedModel, AutoTokenizer, AutoModelWithLMHead\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import re\n",
        "from torch import nn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQic_VP_bwmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#HRbot w/o end and start answer tokens. Answer parts are located in different context sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvGnU1M3uqes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLTqCCpn_WrP",
        "colab_type": "code",
        "outputId": "80de87a8-9eb3-4e34-a8ab-d069ca8680d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "data = pd.read_csv('drive/My Drive/hrbot.csv')\n",
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Какое управленческое действие относится к функ...</td>\n",
              "      <td>планирование, прогнозирование, мотивация, орга...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Управленческий персонал включает:</td>\n",
              "      <td>руководителей, специалистов</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>К функциям менеджмента относят</td>\n",
              "      <td>планирование, прогнозирование, мотивация, орга...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>К японскому менеджменту персонала относится:</td>\n",
              "      <td>продвижение  зависит от возраста рабочего или ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>С какими дисциплинами связана система наук о т...</td>\n",
              "      <td>экономика труда, психология труда, физиология ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question                                             Answer\n",
              "0  Какое управленческое действие относится к функ...  планирование, прогнозирование, мотивация, орга...\n",
              "1                  Управленческий персонал включает:                        руководителей, специалистов\n",
              "2                     К функциям менеджмента относят  планирование, прогнозирование, мотивация, орга...\n",
              "3       К японскому менеджменту персонала относится:  продвижение  зависит от возраста рабочего или ...\n",
              "4  С какими дисциплинами связана система наук о т...  экономика труда, психология труда, физиология ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RXeMsAypCoE",
        "colab_type": "text"
      },
      "source": [
        "Идея - берем hidden state cls берта вопросов, берем hidden state cls берта контекста (учебник), по косинусному расстоянию ищем топ ближайших предложений контекста, для слов из данных предложений берем эмбеддинги с последнего слоя берта, делаем механизм влияния на вопрос, шлем в транформер декодер и решаем задачу абстрактной суммаризации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0TYobWuj9YS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "config_path = '/content/drive/My Drive/Rubert/rubert_cased_L-12_H-768_A-12_pt/bert_config.json'\n",
        "checkpoint_path = '/content/drive/My Drive/Rubert/rubert_cased_L-12_H-768_A-12_pt/pytorch_model.bin'\n",
        "vocab_path = '/content/drive/My Drive/Rubert/rubert_cased_L-12_H-768_A-12_pt/vocab.txt'\n",
        "#data tokenizetion for bert\n",
        "class BotDataset(Dataset):\n",
        "  def __init__(self, data, column, context_atn=False,max_leninf =False):\n",
        "    self.df = data[column].copy()\n",
        "    # self.tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(vocab_path, do_lower_case=True)\n",
        "    # self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
        "    if max_leninf == False:\n",
        "      self.maxlen =  max(data[column].apply(lambda x:len(x)))\n",
        "    else:\n",
        "      self.maxlen = max_leninf\n",
        "    self.context_atn = context_atn\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sentence = self.df.loc[index]\n",
        "    tokens = self.tokenizer.tokenize(sentence)\n",
        "    # if self.context_atn == False:\n",
        "      # tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "    if len(tokens) < self.maxlen:\n",
        "      tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]\n",
        "    else:\n",
        "      if self.context_atn == False:\n",
        "        tokens = tokens[:(self.maxlen-1)] + ['[SEP]']\n",
        "      else: \n",
        "        tokens = tokens[:self.maxlen]\n",
        "    tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "    tokens_ids_tensor = torch.tensor(tokens_ids)\n",
        "    attn_mask = (tokens_ids_tensor != 0).long()\n",
        "    return tokens_ids_tensor, attn_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldtQGH1iKWWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_len = len(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ulJekvZBOJi",
        "colab_type": "code",
        "outputId": "c9c0b0e6-2a4e-4146-857a-39734d6a20be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "border = int(0.8*data_len)\n",
        "Question_dataset_train = BotDataset(data.iloc[:border], 'Question')\n",
        "Question_dataset_test = BotDataset(data.iloc[border:], 'Question')\n",
        "Answer_dataset_train = BotDataset(data.iloc[:border], 'Answer', False, 16)\n",
        "Answer_dataset_test = BotDataset(data.iloc[border:], 'Answer', False, 16)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYeNgI6CMi0M",
        "colab_type": "code",
        "outputId": "b994b30a-87b3-4bf7-d9e6-4e91c56b7b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#Context = hr management issues database\n",
        "context = open('drive/My Drive/context.txt', 'r').read()\n",
        "print(len(context))\n",
        "print(context[:30])\n",
        "pattern = re.compile('[^А-Яа-яЁёA-Za-z0-9/./,/:/?/]')\n",
        "context = pattern.sub(' ', context)\n",
        "context = pd.DataFrame(context.split('.'), columns = ['sentences'])\n",
        "context_len = len(context)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "796689\n",
            "﻿УПРАВЛЕНИЕ ПЕРСОНАЛОМ\n",
            "Под ред\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF9rkCRXV7t1",
        "colab_type": "code",
        "outputId": "7c5e17ad-e694-43e0-e2e8-16184469e902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# context for choosing appropriate sentences for attention \n",
        "context_dataset = BotDataset(context, 'sentences', False,16)\n",
        "\n",
        "# context for answer generation\n",
        "context_dataset_answgen = BotDataset(context, 'sentences', True,16)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WERs-x-UBHJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get CLS embedding \n",
        "# model_qc = BertModel.from_pretrained('bert-base-multilingual-uncased').to(device)\n",
        "model_qc = BertModel.from_pretrained(checkpoint_path, config=config_path).to(device)\n",
        "# model_qc = AutoModelWithLMHead.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\").cuda()\n",
        "model_qc.eval()\n",
        "\n",
        "#question\n",
        "q_tensor_train = torch.stack([Question_dataset_train[x][0] for x in range(border)]).to(device)\n",
        "q_tensor_test = torch.stack([Question_dataset_test[x][0] for x in range(border, data_len)]).to(device)\n",
        "q_atn_train = torch.stack([Question_dataset_train[x][1] for x in range(border)]).to(device)\n",
        "q_atn_test = torch.stack([Question_dataset_test[x][1] for x in range(border, data_len)]).to(device)\n",
        "with torch.no_grad():\n",
        "  q_hid_train, _ = model_qc(q_tensor_train, q_atn_train)\n",
        "  q_hid_test, _ = model_qc(q_tensor_test, q_atn_test)\n",
        "q_hid_train = q_hid_train[:, 0]\n",
        "q_hid_test = q_hid_test[:, 0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0E1mYfQ9qI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_tensor_train = torch.stack([Answer_dataset_train[x][0] for x in range(border)]).to(device)\n",
        "a_tensor_test = torch.stack([Answer_dataset_test[x][0] for x in range(border, data_len)]).to(device)\n",
        "\n",
        "a_tensor_atn_train = torch.stack([Answer_dataset_train[x][1] for x in range(border)]).to(device)\n",
        "a_tensor_atn_test = torch.stack([Answer_dataset_test[x][1] for x in range(border, data_len)]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  a_tensor_train_embed, _ = model_qc(a_tensor_train , a_tensor_atn_train)\n",
        "  # a_tensor_test_embed, _ = model_qc(a_tensor_test , a_tensor_atn_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Uz3oFcOCuih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(q_hid_train, a_tensor_train, a_tensor_atn_train, a_tensor_train_embed)\n",
        "test_dataset = torch.utils.data.TensorDataset(q_hid_test, a_tensor_test, a_tensor_atn_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2gAZdEW7upo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#context for choosing sentence\n",
        "c_tensor = torch.stack([context_dataset[x][0] for x in range(context_len)]).to(device)\n",
        "c_atn = torch.stack([context_dataset[x][1] for x in range(context_len)]).to(device)\n",
        "with torch.no_grad():\n",
        "  c_hid, _ = model_qc(c_tensor, c_atn)\n",
        "  c_hid = c_hid[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvnwYJKHhaUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#context for text generation\n",
        "c_tensor = torch.stack([context_dataset_answgen[x][0] for x in range(context_len)]).to(device)\n",
        "c_atn = torch.stack([context_dataset_answgen[x][1] for x in range(context_len)]).to(device)\n",
        "with torch.no_grad():\n",
        "  w_embed, _ = model_qc(c_tensor, c_atn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mzkk4HGeXlK",
        "colab_type": "code",
        "outputId": "086a2ae3-8a12-4357-8c48-2111d253cce0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "c_tensor[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([14524, 75212,  2068,  9633,  1827, 15034,   867,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiP5_FgHmXbV",
        "colab_type": "code",
        "outputId": "8b5f23b3-7d8b-48af-96f0-6a8afa0404ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "q_hid_train.size()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([544, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E7erfm2h-5V",
        "colab_type": "code",
        "outputId": "dfed4d8d-0709-4a4b-a455-c0f06e2b4148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w_embed.size()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6009, 32, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKcJCM6bnEnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#settings\n",
        "batch_size = 12\n",
        "embedding_dim = q_hid_train.size()[1]\n",
        "attn_size = 16\n",
        "learning_rate = 0.0005"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg1XPBRLXToo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size, shuffle=False, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BkEWWWfMRLp",
        "colab_type": "code",
        "outputId": "05123055-3c81-4039-d2dc-ffb17d3ecf4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained(vocab_path, do_lower_case=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")\n",
        "tokenizer.sep_token_id"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DIBrsmVDrh6J",
        "colab": {}
      },
      "source": [
        "# Source: https://github.com/huggingface/transformers/pull/1455/files  p.s. WIP code, so changes are:  mistakes fixed, prob list added, model changed\n",
        "softm = torch.nn.Softmax(dim=1)\n",
        "class TransformerBeamSearch(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        fc,\n",
        "        vocab_size,\n",
        "        tokenizer,\n",
        "        batch_size,\n",
        "        beam_size,\n",
        "        min_length,\n",
        "        max_length,\n",
        "        alpha=0,\n",
        "        block_repeating_trigram=True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Attributes:\n",
        "            mask_word_id: token id that corresponds to the mask\n",
        "        \"\"\"\n",
        "        super(TransformerBeamSearch, self).__init__()\n",
        "        self.model = model\n",
        "        self.fc = fc\n",
        "        # decoder_config = transformers.AutoConfig.from_pretrained('bert-base-multilingual-uncased', is_decoder=True)\n",
        "        self.end_token_id = tokenizer.sep_token_id\n",
        "        self.start_token_id = tokenizer.cls_token_id\n",
        "        self.beam_size = beam_size\n",
        "        self.min_length = min_length\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.block_repeating_trigram = block_repeating_trigram\n",
        "        self.apply_length_penalty = False if alpha == 0 else True\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # State of the beam\n",
        "        self.hypotheses = [[] for _ in range(batch_size)]\n",
        "        self.batch_offset = torch.arange(batch_size, dtype=torch.long).cuda()\n",
        "        self.beam_offset = torch.arange(\n",
        "            0, batch_size * self.beam_size, step=self.beam_size, dtype=torch.long\n",
        "        ).cuda()\n",
        "        self.growing_beam = torch.full(\n",
        "            (batch_size * self.beam_size, 1), self.start_token_id, dtype=torch.long\n",
        "        )\n",
        "        self.growing_prob = torch.full(\n",
        "            (batch_size,1,self.vocab_size), 0, dtype=torch.long\n",
        "        ).cuda()\n",
        "\n",
        "        self.topk_log_probabilities = torch.tensor(\n",
        "            [0.0] + [float(\"-inf\")] * (self.beam_size - 1), dtype=torch.float\n",
        "        ).repeat(batch_size)\n",
        "        self.results = {\n",
        "            \"predictions\": [[] for _ in range(batch_size)],\n",
        "            \"scores\": [[] for _ in range(batch_size)],\n",
        "            \"probs\": [[] for _ in range(batch_size)],\n",
        "        }\n",
        "        self._step = 0\n",
        "        self.is_done = False\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "      mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "      mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "      return mask \n",
        "\n",
        "    def _generate_subsequent_mask(self, t, s):\n",
        "      mask = (torch.triu(torch.ones(s, t)) == 1).transpose(0, 1)\n",
        "      mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "      return mask      \n",
        "\n",
        "    def step(self, log_probabilities):\n",
        "        \"\"\" Grows the beam by one step. \"\"\"\n",
        "        self._step += 1\n",
        "\n",
        "        # The batch size changes as some beams finish so we define _B\n",
        "        vocab_size = log_probabilities.size(-1)\n",
        "        _B = log_probabilities.size(0) // self.beam_size\n",
        "\n",
        "        # Multiply each beam probability with the probability of the\n",
        "        # next token (conditioned on the words in the beam).\n",
        "      \n",
        "\n",
        "        log_probabilities = log_probabilities.squeeze(1)\n",
        "        \n",
        "        log_probabilities += self.topk_log_probabilities.view(-1, 1).cuda()\n",
        "\n",
        "        log_probabilities = self.enforce_min_length(log_probabilities)\n",
        "        if self.block_repeating_trigram:\n",
        "            self.remove_repeating_trigrams(log_probabilities, _B)\n",
        "\n",
        "       \n",
        "        # Find the `beam_size` (previous_beam + token) combinations with\n",
        "        # the highest score\n",
        "        topk_log_probabilities, topk_ids = torch.topk(\n",
        "            log_probabilities.view(_B, self.beam_size * vocab_size),\n",
        "            self.beam_size,\n",
        "            dim=1) \n",
        "        prob = log_probabilities.view(_B, -1, vocab_size)\n",
        "        prob = torch.max(prob, dim=1).values.unsqueeze(1).long()\n",
        "        prob = (prob >= torch.min(topk_log_probabilities, dim=1).values.unsqueeze(1))*1\n",
        "        \n",
        "        \n",
        "       \n",
        "\n",
        "        # Apply the length penalty. The +1 accounts for the [EOS] token\n",
        "        # that will be added if the beam ends.\n",
        "        topk_scores = topk_log_probabilities / self.length_penalty()\n",
        "        \n",
        "        # Retrieve the corresponding respective beam and token id\n",
        "        # topk_token_ids[i] will be added to topk_beam_ids[i]\n",
        "        topk_beam_ids = topk_ids.div(vocab_size)\n",
        "        topk_token_ids = topk_ids.fmod(vocab_size)\n",
        "        \n",
        "       \n",
        "        # Retrieve the row index of the surviving beams in the original\n",
        "        # view of the log_probabilities tensor\n",
        "        surviving_beams_rows = (topk_beam_ids + self.beam_offset[:_B].view(-1, 1).cuda()).view(\n",
        "            -1\n",
        "        )\n",
        "\n",
        "        # Append the last predictions\n",
        "        self.growing_beam = torch.cat(\n",
        "            [\n",
        "                self.growing_beam.index_select(0, surviving_beams_rows),\n",
        "                topk_token_ids.view(-1, 1),\n",
        "            ],\n",
        "            1,\n",
        "        )\n",
        "        self.growing_prob = torch.cat(\n",
        "            [\n",
        "                self.growing_prob,\n",
        "                prob.cuda(),\n",
        "            ],\n",
        "            1,\n",
        "        )\n",
        "\n",
        "\n",
        "        # Check if any of the beam searches has ended during this\n",
        "        # growth step. Also if top beam (most probable) has ended\n",
        "        # for one element of the batch.\n",
        "        \n",
        "        is_finished = topk_token_ids.eq(self.end_token_id)\n",
        "       \n",
        "        is_finished = self.enforce_max_length(is_finished)\n",
        "        \n",
        "        is_top_beam_finished = is_finished[:, 0].eq(1)\n",
        "\n",
        "        # Save the finished searches\n",
        "        \n",
        "        if is_finished.any():\n",
        "            predictions = self.growing_beam.view(-1, self.beam_size, self.growing_beam.size(1))\n",
        "            for i in range(is_finished.size(0)):\n",
        "                if is_top_beam_finished[i]:\n",
        "                    is_finished[i].fill_(1)\n",
        "                finished_hyp = is_finished[i].nonzero().view(-1)\n",
        "\n",
        "                # Store finished hypotheses for this batch.\n",
        "                b = self.batch_offset[i]\n",
        "                for j in finished_hyp:\n",
        "                    self.hypotheses[b].append((topk_scores[i, j], predictions[i, j, :]))\n",
        "\n",
        "                # If the batch reached the end, save the best hypotheses\n",
        "                # in terms of length-penalized score.\n",
        "                if is_top_beam_finished[i]:\n",
        "                    best_hyp = sorted(\n",
        "                        self.hypotheses[b], key=lambda x: x[0], reverse=True\n",
        "                    )\n",
        "                    best_score, best_prediction = best_hyp[0]\n",
        "                    self.results[\"scores\"][b].append(best_score)\n",
        "                    self.results[\"predictions\"][b].append(best_prediction)\n",
        "                    self.results[\"probs\"][b].append(self.growing_prob[b])\n",
        "\n",
        "            non_finished = is_top_beam_finished.eq(0).nonzero().view(-1).cuda()\n",
        "            if len(non_finished) == 0:\n",
        "                self.is_done = True\n",
        "\n",
        "            # Remove finished batches for the next step.\n",
        "            topk_log_probabilities = topk_log_probabilities.index_select(\n",
        "                0, non_finished\n",
        "            )\n",
        "            self.batch_offset = self.batch_offset.index_select(0, non_finished)\n",
        "            self.growing_beam = predictions.index_select(0, non_finished).view(\n",
        "                -1, self.growing_beam.size(-1)\n",
        "            )\n",
        "\n",
        "            surviving_beams_rows = surviving_beams_rows.index_select(0, non_finished)\n",
        "\n",
        "        return surviving_beams_rows\n",
        "\n",
        "    def forward(self, encoder_outputs,encoder_embeddings):\n",
        "        # keyword arguments come in 3 flavors: encoder-specific (prefixed by\n",
        "        # `encoder_`), decoder-specific (prefixed by `decoder_`) and those\n",
        "        # that apply to the model as whole.\n",
        "        # We let the specific kwargs override the common ones in case of conflict.\n",
        "        # kwargs_encoder = {\n",
        "        #     argument[len(\"encoder_\"):]: value\n",
        "        #     for argument, value in kwargs.items()\n",
        "        #     if argument.startswith(\"encoder_\")\n",
        "        # }\n",
        "        # kwargs_decoder = {\n",
        "        #     argument[len(\"decoder_\"):]: value\n",
        "        #     for argument, value in kwargs.items()\n",
        "        #     if argument.startswith(\"decoder_\")\n",
        "        # }\n",
        "        # kwargs_common = {\n",
        "        #     argument: value\n",
        "        #     for argument, value in kwargs.items()\n",
        "        #     if not (argument.startswith(\"encoder_\") or argument.startswith(\"decoder_\"))\n",
        "        # }\n",
        "        # kwargs_decoder = dict(kwargs_common, **kwargs_decoder)\n",
        "        # kwargs_encoder = dict(kwargs_common, **kwargs_encoder)\n",
        "\n",
        "        # forward pass on the encoder\n",
        "  \n",
        "        encoder_embeddings = encoder_embeddings.permute(1,0,2)\n",
        "        encoder_hidden_states = tile(\n",
        "            encoder_embeddings, self.beam_size, dim=0, variants=1,\n",
        "        )\n",
        "        encoder_outputs = tile(\n",
        "            encoder_outputs, self.beam_size, dim=0, variants=2,\n",
        "        )\n",
        "\n",
        "        \n",
        "        \n",
        "        # grow the beam by generating sequences in an autoregressive way\n",
        "        self.growing_beam = torch.full(\n",
        "            (self.batch_size * self.beam_size, 1), self.start_token_id, dtype=torch.long).cuda()\n",
        "        for step in range(self.max_length):\n",
        "            decoder_input = self.growing_beam\n",
        "            # [:, -1]\n",
        "            # decoder_input = decoder_input.view(-1,1)\n",
        "            decoder_input_atn = ((decoder_input == 0)*1).cuda()\n",
        "            \n",
        "            attn_mask = ((encoder_outputs == 0)*1).cuda()\n",
        "           \n",
        "           \n",
        "            with torch.no_grad():\n",
        "               decoder_input, _ = model_qc(decoder_input , decoder_input_atn)\n",
        "            decoder_input_ch = decoder_input.permute(1,0,2)\n",
        "            encoder_hidden_states = encoder_hidden_states.permute(1,0,2)\n",
        "          \n",
        "            mask = self._generate_subsequent_mask(decoder_input_ch.size(0),encoder_hidden_states.size(0)).to(device)\n",
        "            mask_targ = self._generate_square_subsequent_mask(len(decoder_input_ch)).to(device)\n",
        "            \n",
        "            outputs = self.model(decoder_input_ch.cuda(), encoder_hidden_states.cuda(), mask_targ ,mask.cuda(),decoder_input_atn.bool(), attn_mask.bool() )\n",
        "            outputs = outputs[-1].unsqueeze(0)\n",
        "            \n",
        "            outputs = outputs.permute(1,0,2)\n",
        "            outputs = self.fc(outputs)\n",
        "            \n",
        "            \n",
        "            log_probabilities = torch.nn.functional.log_softmax(outputs)\n",
        "           \n",
        "            surviving_beams_rows = self.step(log_probabilities)\n",
        "            if self.is_done:\n",
        "                break\n",
        "            encoder_hidden_states = encoder_hidden_states.permute(1,0,2)\n",
        "            encoder_hidden_states = encoder_hidden_states.index_select(0, surviving_beams_rows)\n",
        "            encoder_outputs = encoder_outputs.index_select(0, surviving_beams_rows)\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def remove_repeating_trigrams(self, log_probabilities, _B):\n",
        "        if(self._step + 1 > 1):\n",
        "            for i in range(_B * self.beam_size):\n",
        "                tokens = [t for t in self.growing_beam[i]]\n",
        "                if(self._step + 1 > 3):\n",
        "                  trigrams = [(tokens[k-1], tokens[k], tokens[k+1]) for k in range(1, len(tokens) - 1)]\n",
        "                  last_trigram = tuple(trigrams[-1])\n",
        "                  if last_trigram in trigrams[:-1]:\n",
        "                      log_probabilities[i] = -1e20\n",
        "        \n",
        "                if tokens[-1] in tokens[:-1]:\n",
        "                    log_probabilities[i] = -1e20\n",
        "                \n",
        "\n",
        "    def enforce_min_length(self, log_probabilities):\n",
        "        if self._step < self.min_length:\n",
        "            log_probabilities[:, self.end_token_id] = -1e20\n",
        "        return log_probabilities\n",
        "\n",
        "    def enforce_max_length(self, is_finished):\n",
        "        if self._step + 1 == self.max_length:\n",
        "            is_finished.fill_(1)\n",
        "        return is_finished\n",
        "\n",
        "    def length_penalty(self):\n",
        "        return ((5.0 + (self._step + 1)) / 6.0) ** self.alpha\n",
        "\n",
        "\n",
        "def tile(x, count, dim=0, variants=1):\n",
        "    \"\"\"\n",
        "    Tiles `x` along dimension `dim` `count` times.\n",
        "    Example:\n",
        "        >> ex = torch.tensor([1,2],[3,4])\n",
        "        >> tile(ex, 2, 0)\n",
        "        torch.Tensor([[1,2],[1,2],[3,4],[3,4]])\n",
        "    \"\"\"\n",
        "    perm = list(range(len(x.size())))\n",
        "    if dim != 0:\n",
        "        if variants == 1:\n",
        "          perm[0], perm[dim], perm[2] = perm[dim], perm[0], perm[2]\n",
        "        else:\n",
        "          perm[0], perm[dim] = perm[dim], perm[0]\n",
        "        x = x.permute(perm).contiguous()\n",
        "    out_size = list(x.size())\n",
        "    out_size[0] *= count\n",
        "    batch = x.size(0)\n",
        "    if variants == 1:\n",
        "      x = (\n",
        "          x.view(batch, -1, 768)\n",
        "          .transpose(0, 1)\n",
        "          .repeat(count, 1, 1)\n",
        "          .transpose(0, 1)\n",
        "          .contiguous()\n",
        "          .view(*out_size)\n",
        "      )\n",
        "    else:\n",
        "      x = (\n",
        "          x.view(batch, -1)\n",
        "          .transpose(0, 1)\n",
        "          .repeat(count, 1)\n",
        "          .transpose(0, 1)\n",
        "          .contiguous()\n",
        "          .view(*out_size)\n",
        "      )      \n",
        "    if dim != 0:\n",
        "        x = x.permute(perm).contiguous()\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWWYfDzjaQ3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "class HRbotAnswerGen(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(HRbotAnswerGen, self).__init__()\n",
        "    # self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(vocab_path, do_lower_case=True)\n",
        "    # self.tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")\n",
        "    # decoder_config = transformers.AutoConfig.from_pretrained('bert-base-multilingual-uncased', is_decoder=True)\n",
        "    # self.model_gen = Model2Model.from_pretrained('bert-base-multilingual-uncased', decoder_config=decoder_config)\n",
        "    self.Q = nn.Linear(embedding_dim, attn_size)\n",
        "    torch.nn.init.xavier_uniform(self.Q.weight)\n",
        "    self.K = nn.Linear(embedding_dim, attn_size)\n",
        "    torch.nn.init.xavier_uniform(self.K.weight)\n",
        "    self.V = nn.Linear(embedding_dim, embedding_dim)\n",
        "    torch.nn.init.xavier_uniform(self.V.weight)\n",
        "    self.decoder_layer = nn.TransformerDecoderLayer(d_model=768, nhead=12)\n",
        "    self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=8)\n",
        "    # self.embed = nn.Embedding(self.tokenizer.vocab_size,embedding_dim)\n",
        "    self.fc = nn.Linear(embedding_dim, self.tokenizer.vocab_size)\n",
        "    torch.nn.init.xavier_uniform(self.fc.weight)\n",
        "\n",
        "  def _generate_square_subsequent_mask(self, sz):\n",
        "      mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "      mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "      return mask \n",
        "\n",
        "  def _generate_subsequent_mask(self, t, s):\n",
        "      mask = (torch.triu(torch.ones(s, t)) == 1).transpose(0, 1)\n",
        "      mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "      return mask  \n",
        "    \n",
        "  def forward(self, w_embed, q_hid_batch, c_hid, c_tensor, c_atn, a_tensor = None, a_tensor_atn = None, test = False, a_tensor_embed = None):\n",
        "    \n",
        "    q = 0\n",
        "    for question in q_hid_batch:\n",
        "      cos_q_tensor = question.repeat(c_hid.size(0),1)\n",
        "      cos_qc_tensor = cos(cos_q_tensor, c_hid)\n",
        "      _, attn_index = torch.sort(cos_qc_tensor, descending=True)\n",
        "      attn_index = attn_index[:10]\n",
        "      for index in attn_index:\n",
        "        w_embed_at = w_embed[index]\n",
        "        c_tensor_at = c_tensor[index]\n",
        "        # c_atn_at = c_atn[index]\n",
        "        if attn_index[0] == index:\n",
        "          w_embed_at_total = w_embed_at.clone()\n",
        "          c_tensor_at_total = c_tensor_at.clone()\n",
        "          # c_atn_at_total = c_atn_at.clone()\n",
        "        else:\n",
        "          w_embed_at_total = torch.cat([w_embed_at_total, w_embed_at], dim=0)\n",
        "          c_tensor_at_total = torch.cat([c_tensor_at_total, c_tensor_at], dim=0)\n",
        "          # c_atn_at_total = torch.cat([c_atn_at_total, c_atn_at], dim=0)\n",
        "      if q == 0:\n",
        "        question_total = w_embed_at_total.unsqueeze(0).clone()\n",
        "        question_context = c_tensor_at_total.unsqueeze(0).clone()\n",
        "        # question_c_atn = c_atn_at_total.unsqueeze(0).clone()\n",
        "        q = q + 1\n",
        "      else:\n",
        "        question_total = torch.cat([question_total, w_embed_at_total.unsqueeze(0)], dim=0)\n",
        "        question_context = torch.cat([question_context, c_tensor_at_total.unsqueeze(0)], dim=0)\n",
        "        # question_c_atn = torch.cat([question_c_atn, c_atn_at_total.unsqueeze(0)], dim=0)    \n",
        "\n",
        "    q_hid_batch = q_hid_batch.unsqueeze(1)\n",
        "    Q_question = self.Q(q_hid_batch)\n",
        "    K_wembed = self.K(question_total)\n",
        "    V_embedgen = self.V(question_total)\n",
        "    softm_val = softm(torch.matmul(Q_question, K_wembed.permute(0,2,1))/(attn_size**(1/2)))\n",
        "    softm_val = softm_val.squeeze(1)\n",
        "\n",
        "\n",
        "    # --------for embedding input--------------------------------\n",
        "    softm_matrix = torch.zeros(question_total.size(1),question_total.size(1)).repeat(batch_size,1,1).to(device)\n",
        "    for i, _ in enumerate(softm_matrix):\n",
        "      softm_matrix[i][torch.eye(question_total.size(1)).byte()] =softm_val[i]\n",
        "    question_context_embed = torch.matmul(softm_matrix, V_embedgen)\n",
        "    \n",
        "\n",
        "\n",
        "    #----------for index input----------------------------------\n",
        "    # softm_val = softm_val/torch.max(softm_val, dim=1).values.unsqueeze(1)\n",
        "    # softm_val = torch.round(softm_val)\n",
        "    # question_context = softm_val * question_context\n",
        "   \n",
        "    # self.model_gen.encoder.requires_grad_=False\n",
        "    # self.model_gen.decoder.requires_grad_=False\n",
        "    # self.model_gen.decoder.cls.requires_grad_=True\n",
        "\n",
        "    \n",
        "    del cos_q_tensor\n",
        "    del cos_qc_tensor\n",
        "    del w_embed_at_total\n",
        "    del c_tensor_at_total\n",
        "    # del c_atn_at_total\n",
        "    del w_embed_at\n",
        "    del c_tensor_at\n",
        "    # del c_atn_at  \n",
        "    del question_total\n",
        "    # del question_c_atn\n",
        "    \n",
        "    question_context_embed = question_context_embed.permute(1,0,2)\n",
        "    # question_context = question_context.permute(1,0)\n",
        "    if a_tensor_embed != None:\n",
        "      a_tensor_embed = a_tensor_embed.permute(1,0,2)\n",
        "      a_tensor_atn = ((a_tensor_atn == 0)*1).to(device)\n",
        "    # a_tensor_atn = a_tensor_atn.permute(1,0)\n",
        "\n",
        "    question_context_atn = ((question_context == 0)*1).to(device)\n",
        "    torch.cuda.empty_cache()\n",
        "   \n",
        "\n",
        "    if test == False:\n",
        "      mask = self._generate_subsequent_mask(a_tensor_embed.size(0),question_context_embed.size(0)).to(device)\n",
        "      mask_targ = self._generate_square_subsequent_mask(a_tensor_embed.size(0)).to(device)\n",
        "      # a_tensor = self.embed(a_tensor)\n",
        "      output = self.transformer_decoder(a_tensor_embed,question_context_embed,mask_targ ,mask,a_tensor_atn.bool(),question_context_atn.bool())\n",
        "      output = output.permute(1,0,2)\n",
        "      output = self.fc(output)\n",
        "      \n",
        "      # model_kwargs = {\"encoder_attention_mask\": question_context_atn, \"decoder_attention_mask\": a_tensor_atn }\n",
        "      # out, _ , _= self.model_gen(encoder_input_ids=question_context.long(),decoder_input_ids=a_tensor.long(), **model_kwargs)\n",
        "      # model.encoder.layer[3].attention.self.key.weight.reqieres_grad = True\n",
        "      return output\n",
        "    else:\n",
        "      # model_kwargs = {\"encoder_attention_mask\": question_context_atn}\n",
        "      tran_beams = TransformerBeamSearch(self.transformer_decoder, self.fc,self.tokenizer.vocab_size,self.tokenizer, batch_size = q_hid_batch.size(0),beam_size=5, min_length = 16 ,max_length = 16)\n",
        "      \n",
        "      out = tran_beams.forward(question_context, question_context_embed)\n",
        "      score = torch.stack([x[0] for x in out['scores']])\n",
        "      \n",
        "      prob = torch.stack([x[0] for x in out['probs']])\n",
        "   \n",
        "      pred = torch.stack([x[0] for x in out['predictions']])\n",
        "      return score, pred, prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epcReKG3xMrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tgt – the sequence to the decoder (required).\n",
        "\n",
        "# memory – the sequnce from the last layer of the encoder (required).\n",
        "\n",
        "# tgt_mask – the mask for the tgt sequence (optional).\n",
        "\n",
        "# memory_mask – the mask for the memory sequence (optional).\n",
        "\n",
        "# tgt_key_padding_mask – the mask for the tgt keys per batch (optional).\n",
        "\n",
        "# memory_key_padding_mask – the mask for the memory keys per batch (optional)."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u7HYJf49ddc",
        "colab_type": "code",
        "outputId": "2ad3d3eb-5c53-46de-d4f7-eaa2ca4c21eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = HRbotAnswerGen()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewxo5tYf926d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_05Tz-Y-JwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
        "    total_loss = 0\n",
        "  \n",
        "    for i in range(len(real[0])):\n",
        "        \n",
        "      mask = real[:,i].ge(1).type(torch.cuda.FloatTensor)\n",
        "      crit = nn.CrossEntropyLoss()\n",
        "      loss_ = crit(pred[:,i].type(torch.cuda.FloatTensor), real[:,i].long()) * mask \n",
        "      total_loss  =  total_loss + torch.mean(loss_)\n",
        "    return total_loss/len(real[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRg-6bKDGV55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def evaluate(model, test_loader, device, c_hid, c_tensor, c_atn, w_embed):\n",
        "  model.eval()\n",
        "\n",
        "  for i, (q_hid_batch_test, answer_batch_test, answer_atn_batch_test) in enumerate(test_loader):\n",
        "    q_hid_batch_test, answer_batch_test, answer_atn_batch_test = q_hid_batch_test.to(device), answer_batch_test.to(device), answer_atn_batch_test.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "      test_output = model(w_embed, q_hid_batch_test, c_hid, c_tensor, c_atn, test=True)\n",
        "    \n",
        "    #use scores as logits\n",
        "    \n",
        "    test_loss = loss_function(answer_batch_test, test_output[2])\n",
        "    return test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3d0jC9j87IQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, test_loader, device, epochs,c_hid, c_tensor, c_atn, w_embed):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  c_hid, c_tensor, c_atn, w_embed = c_hid.to(device), c_tensor.to(device), c_atn.to(device), w_embed.to(device)\n",
        "  optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    total_loss = 0\n",
        "    for i , (q_hid_batch, answer_batch, answer_atn_batch, a_tensor_train_embed) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      q_hid_batch, answer_batch, answer_atn_batch, a_tensor_train_embed = q_hid_batch.to(device), answer_batch.to(device), answer_atn_batch.to(device), a_tensor_train_embed.to(device)\n",
        "\n",
        "      logits = model(w_embed, q_hid_batch, c_hid, c_tensor, c_atn, answer_batch, answer_atn_batch, False ,a_tensor_train_embed)\n",
        "\n",
        "      loss = loss_function(answer_batch, logits)\n",
        "      loss.backward()\n",
        "      total_loss = total_loss + loss\n",
        "     \n",
        "      nn.utils.clip_grad_norm_(model.parameters(),0.5)\n",
        "\n",
        "      optimizer.step()\n",
        "      \n",
        "    val_loss = evaluate(model, test_loader, device, c_hid, c_tensor, c_atn, w_embed)\n",
        "    print(f'Epoch {epoch}, Train_loss: {total_loss/(i+1)}, Val_loss: {val_loss}')\n",
        "     \n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2F6aYY63d30",
        "colab_type": "code",
        "outputId": "a4fb53c2-704f-4f7d-b457-d32d7081515b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "model = train(model, train_loader, test_loader, device, epochs,c_hid, c_tensor, c_atn, w_embed)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Train_loss: 4.800919532775879, Val_loss: 6.027108669281006\n",
            "Epoch 1, Train_loss: 3.3890469074249268, Val_loss: 6.017892837524414\n",
            "Epoch 2, Train_loss: 3.0446395874023438, Val_loss: 5.944858551025391\n",
            "Epoch 3, Train_loss: 2.972137928009033, Val_loss: 5.950615406036377\n",
            "Epoch 4, Train_loss: 2.794565200805664, Val_loss: 5.969342231750488\n",
            "Epoch 5, Train_loss: 2.5294294357299805, Val_loss: 6.014833450317383\n",
            "Epoch 6, Train_loss: 2.282773971557617, Val_loss: 6.0149736404418945\n",
            "Epoch 7, Train_loss: 1.9672931432724, Val_loss: 6.016326427459717\n",
            "Epoch 8, Train_loss: 1.7333730459213257, Val_loss: 6.019829273223877\n",
            "Epoch 9, Train_loss: 1.5785183906555176, Val_loss: 5.99925422668457\n",
            "Epoch 10, Train_loss: 1.4628888368606567, Val_loss: 6.005346298217773\n",
            "Epoch 11, Train_loss: 1.3636518716812134, Val_loss: 6.044302940368652\n",
            "Epoch 12, Train_loss: 1.3272987604141235, Val_loss: 6.007230758666992\n",
            "Epoch 13, Train_loss: 1.238893747329712, Val_loss: 6.025773525238037\n",
            "Epoch 14, Train_loss: 1.220080018043518, Val_loss: 6.023091793060303\n",
            "Epoch 15, Train_loss: 1.1807348728179932, Val_loss: 5.98844575881958\n",
            "Epoch 16, Train_loss: 1.15314519405365, Val_loss: 6.017399787902832\n",
            "Epoch 17, Train_loss: 1.107505202293396, Val_loss: 6.010293483734131\n",
            "Epoch 18, Train_loss: 1.0998317003250122, Val_loss: 6.027910232543945\n",
            "Epoch 19, Train_loss: 1.064710021018982, Val_loss: 6.012600898742676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp1vE0Qy-uAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        " with torch.no_grad():\n",
        "      check_logits = model(w_embed, q_hid_test[-48:-36], c_hid, c_tensor, c_atn, test=True)\n",
        "   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2RVN2z3s4-H",
        "colab_type": "code",
        "outputId": "31785a02-3adf-4b5f-c897-4bb7a3504cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tokenizer.convert_ids_to_tokens(check_logits[1][11]))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'образование', 'лока', 'участка', 'способных', '##ше', '##ше', '[unused1]', '##анию', 'оценок', '##анию', '[unused1]', '[unused1]', '[unused1]', '[unused1]', '[unused1]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woV6MqCVPbo_",
        "colab_type": "code",
        "outputId": "e0525c90-4211-4b39-ca7b-481c338cd675",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "data.iloc[-36]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Question                           Развитие персонала - это: \n",
              "Answer      процесс подготовки сотрудника к выполнению нов...\n",
              "Name: 644, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1tsTxNRivDk",
        "colab_type": "text"
      },
      "source": [
        "Sentence has no sence at the moment. what can we do next - use different batch size (batch is too small), not enough data - use Russian pretrained decoder, maybe should use ideas from T5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BiQZwzu6nru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model\n",
        "del check_logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSCuMxYO_sWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")\n",
        "vocab_size = tokenizer.vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q27bd52_9oT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "393a87bf-367e-4b98-eaa8-854ae59306ef"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119547"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvjSt2rSfl_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#другая попытка -  взять предобученный на соц сетях руберт и попытаться его исп. Берт плохо подходит для генерации текста, но необходимо было проверить"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh7JqT0XDu43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_gen = AutoModelWithLMHead.from_pretrained(\"DeepPavlov/rubert-base-cased-conversational\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2OmXwyULPGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softm = torch.nn.Softmax(dim=-1)\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "class HRbotAnswerGen(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(HRbotAnswerGen, self).__init__()\n",
        "\n",
        "    # self.tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")\n",
        "\n",
        "    self.Q = nn.Linear(embedding_dim, attn_size)\n",
        "    torch.nn.init.xavier_uniform(self.Q.weight)\n",
        "    self.K = nn.Linear(embedding_dim, attn_size)\n",
        "    torch.nn.init.xavier_uniform(self.K.weight)\n",
        "    self.V = nn.Linear(embedding_dim, embedding_dim)\n",
        "    torch.nn.init.xavier_uniform(self.V.weight)\n",
        "    self.qc = model_qc\n",
        "    # self.embed = nn.Embedding(self.tokenizer.vocab_size,embedding_dim)\n",
        "    \n",
        "\n",
        "    self.model_gen = model_gen\n",
        "\n",
        "    \n",
        "  def forward(self, w_embed, q_hid_batch, c_hid, c_tensor, c_atn, a_tensor = None, a_tensor_atn = None, test = False, a_tensor_embed = None):\n",
        "    \n",
        "    q = 0\n",
        "    for question in q_hid_batch:\n",
        "      cos_q_tensor = question.repeat(c_hid.size(0),1)\n",
        "      cos_qc_tensor = cos(cos_q_tensor, c_hid)\n",
        "      _, attn_index = torch.sort(cos_qc_tensor, descending=True)\n",
        "      attn_index = attn_index[:10]\n",
        "      for index in attn_index:\n",
        "        w_embed_at = w_embed[index]\n",
        "        c_tensor_at = c_tensor[index]\n",
        "        # c_atn_at = c_atn[index]\n",
        "        if attn_index[0] == index:\n",
        "          w_embed_at_total = w_embed_at.clone()\n",
        "          c_tensor_at_total = c_tensor_at.clone()\n",
        "          # c_atn_at_total = c_atn_at.clone()\n",
        "        else:\n",
        "          w_embed_at_total = torch.cat([w_embed_at_total, w_embed_at], dim=0)\n",
        "          c_tensor_at_total = torch.cat([c_tensor_at_total, c_tensor_at], dim=0)\n",
        "          # c_atn_at_total = torch.cat([c_atn_at_total, c_atn_at], dim=0)\n",
        "      if q == 0:\n",
        "        question_total = w_embed_at_total.unsqueeze(0).clone()\n",
        "        question_context = c_tensor_at_total.unsqueeze(0).clone()\n",
        "        # question_c_atn = c_atn_at_total.unsqueeze(0).clone()\n",
        "        q = q + 1\n",
        "      else:\n",
        "        question_total = torch.cat([question_total, w_embed_at_total.unsqueeze(0)], dim=0)\n",
        "        question_context = torch.cat([question_context, c_tensor_at_total.unsqueeze(0)], dim=0)\n",
        "        # question_c_atn = torch.cat([question_c_atn, c_atn_at_total.unsqueeze(0)], dim=0)    \n",
        "\n",
        "    q_hid_batch = q_hid_batch.unsqueeze(1)\n",
        "    Q_question = self.Q(q_hid_batch)\n",
        "    K_wembed = self.K(question_total)\n",
        "    V_embedgen = self.V(question_total)\n",
        "    softm_val = softm(torch.matmul(Q_question, K_wembed.permute(0,2,1))/(attn_size**(1/2)))\n",
        "    softm_val = softm_val.squeeze(1)\n",
        "\n",
        "\n",
        "    # --------for embedding input--------------------------------\n",
        "    # softm_matrix = torch.zeros(question_total.size(1),question_total.size(1)).repeat(batch_size,1,1).to(device)\n",
        "    # for i, _ in enumerate(softm_matrix):\n",
        "    #   softm_matrix[i][torch.eye(question_total.size(1)).byte()] =softm_val[i]\n",
        "    # question_context_embed = torch.matmul(softm_matrix, V_embedgen)\n",
        "    \n",
        "\n",
        "\n",
        "    #----------for index input----------------------------------\n",
        "    softm_val = softm_val/torch.max(softm_val, dim=1).values.unsqueeze(1)\n",
        "    softm_val = torch.round(softm_val)\n",
        "    question_context = softm_val * question_context\n",
        "   \n",
        "    # self.model_gen.encoder.requires_grad_=False\n",
        "    # self.model_gen.decoder.requires_grad_=False\n",
        "    # self.model_gen.decoder.cls.requires_grad_=True\n",
        "\n",
        "    \n",
        "    del cos_q_tensor\n",
        "    del cos_qc_tensor\n",
        "    del w_embed_at_total\n",
        "    del c_tensor_at_total\n",
        "    # del c_atn_at_total\n",
        "    del w_embed_at\n",
        "    del c_tensor_at\n",
        "    # del c_atn_at  \n",
        "    \n",
        "    # del question_c_atn\n",
        "    \n",
        "  \n",
        "    torch.cuda.empty_cache()\n",
        "   \n",
        "   \n",
        "    question_context_atn = (question_context != 0)*1\n",
        "  \n",
        "  \n",
        "    output  =  self.model_gen(question_context.long(), question_context_atn.long())[0]\n",
        "    output = softm(output)\n",
        "    output=torch.argmax(output, dim=-1)\n",
        "    output_emb, _ = self.qc(output, (output!=0)*1)\n",
        " \n",
        "    return output, output_emb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGlNgjzTR5vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = HRbotAnswerGen()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OaM5yqXF2aP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs =3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBhK-r5zSA31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CosineEmbeddingLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJosjV_ClfLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, test_loader, device, c_hid, c_tensor, c_atn, w_embed):\n",
        "  model.eval()\n",
        "\n",
        "  for i, (q_hid_batch_test, answer_batch_test, answer_atn_batch_test) in enumerate(test_loader):\n",
        "    q_hid_batch_test, answer_batch_test, answer_atn_batch_test = q_hid_batch_test.to(device), answer_batch_test.to(device), answer_atn_batch_test.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "      test_output = model(w_embed, q_hid_batch_test, c_hid, c_tensor, c_atn, test=True)[1]\n",
        "      test_output = test_output[:,0]\n",
        "      answer_batch_test, _ = model_qc(answer_batch_test, answer_atn_batch_test)\n",
        "      answer_batch_test = answer_batch_test[:,0]\n",
        "    #use scores as logits\n",
        "    \n",
        "    test_loss = criterion(test_output, answer_batch_test, torch.tensor([1]*len(test_output)).cuda())\n",
        "    return test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGqp3hAxrmH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, test_loader, device, epochs,c_hid, c_tensor, c_atn, w_embed):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  c_hid, c_tensor, c_atn, w_embed = c_hid.to(device), c_tensor.to(device), c_atn.to(device), w_embed.to(device)\n",
        "  optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    total_loss = 0\n",
        "    for i , (q_hid_batch, answer_batch, answer_atn_batch, a_tensor_train_embed) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      q_hid_batch, answer_batch, answer_atn_batch, a_tensor_train_embed = q_hid_batch.to(device), answer_batch.to(device), answer_atn_batch.to(device), a_tensor_train_embed.to(device)\n",
        "\n",
        "      \n",
        "      logits = model(w_embed, q_hid_batch, c_hid, c_tensor, c_atn, answer_batch, answer_atn_batch, False ,a_tensor_train_embed)[1]\n",
        "\n",
        "      \n",
        "      \n",
        "      logits_emb = logits[:,0]\n",
        "      a_tensor_train_embed = a_tensor_train_embed[:, 0]\n",
        "\n",
        "      loss = criterion(logits_emb, a_tensor_train_embed, torch.tensor([1]*len(logits_emb)).cuda())\n",
        "      loss.backward()\n",
        "      total_loss = total_loss + loss\n",
        "     \n",
        "      nn.utils.clip_grad_norm_(model.parameters(),0.5)\n",
        "\n",
        "      optimizer.step()\n",
        "      \n",
        "    val_loss = evaluate(model, test_loader, device, c_hid, c_tensor, c_atn, w_embed)\n",
        "    print(f'Epoch {epoch}, Train_loss: {total_loss/(i+1)}, Val_loss: {val_loss}')\n",
        "     \n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL-efArCGTAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#settings\n",
        "batch_size = 4\n",
        "embedding_dim = q_hid_train.size()[1]\n",
        "attn_size = 16\n",
        "learning_rate = 0.0005"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZAzd-RMtgVh",
        "colab_type": "code",
        "outputId": "7fc38f43-64af-4296-e97c-fe221c284b37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "model = train(model, train_loader, test_loader, device, epochs,c_hid, c_tensor, c_atn, w_embed)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Train_loss: 0.335764616727829, Val_loss: -8.940696716308594e-08\n",
            "Epoch 1, Train_loss: 0.3121093809604645, Val_loss: -5.960464477539063e-08\n",
            "Epoch 2, Train_loss: 0.310528427362442, Val_loss: -2.9802322387695312e-08\n",
            "Epoch 3, Train_loss: 0.3097914457321167, Val_loss: -1.4901161193847656e-08\n",
            "Epoch 4, Train_loss: 0.3093159794807434, Val_loss: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TanTLRKBNDH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        " with torch.no_grad():\n",
        "      check_logits = model(w_embed, q_hid_train[-40:-36], c_hid, c_tensor, c_atn, test=True)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtjPNkc_NVYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "db8e4f52-74e8-4b66-8ce8-5085914f4819"
      },
      "source": [
        "print(tokenizer.convert_ids_to_tokens(check_logits[3]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['верет', '##eng', 'остан', '##gu', '##gu', 'истины', 'истины', 'Пуч', 'уголовно', '##come', '##र', '##र', '##र', '##र', '##come', '##र', 'столько', 'Пуч', '##нуют', '##нуют', 'Пуч', '##нуют', '##oci', 'Пуч', 'уголовно', '##oci', '##come', '##र', '##र', '##र', '##र', 'столько', 'бульдоз', 'наночаст', 'наночаст', 'открыло', 'наночаст', 'наночаст', 'наночаст', 'наночаст', 'открыло', 'вят', '##eng', '##र', '##र', '##र', '##र', '##र', '##र', '##eng', 'остан', 'наночаст', 'наночаст', 'наночаст', 'открыло', 'открыло', 'открыло', 'личное', 'калмыков', '##र', '##र', '##र', '##र', '##र', '##र', '##र', '##eng', 'бульдоз', 'открыло', 'открыло', 'открыло', 'личное', 'личное', 'личное', '##र', '##र', 'объективов', 'волостей', 'волостей', 'волостей', 'волостей', 'волостей', 'волостей', 'медитации', '##gu', 'медитации', 'медитации', 'почетный', 'Обыч', 'Обыч', 'почетный', 'Пуч', '##арра', 'географическом', 'действий', 'Колчака', 'Колчака', 'волостей', '##ыша', 'телевизионной', '##иваемые', 'Шаховская', '##иваемые', '##иваемые', 'редкостью', 'редкостью', 'преступления', 'преступления', 'Миша', 'Миша', '##pre', '##pre', 'действий', '##стана', '##стана', 'благородного', 'благородного', '##иваемые', '##иваемые', 'Шаховская', '##ыша', 'Шаховская', 'преступления', 'преступления', 'преступления', 'преступления', 'Шаховская', 'ссыльных', '##pre', '##OX', '##стана', 'благородного', 'благородного', 'благородного', '##иваемые', 'Шаховская', '##иваемые', '##иваемые', 'Шаховская', 'преступления', '##ные', 'Пуч', 'уголовно', 'уголовно', '##oci', 'Пуч', '##come', '##come', '##र', '##र', '##र', '##र', 'столько', '##eng', 'Шаховская', '##иваемые', '##иваемые', 'личное', 'личное', 'личное']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbXG7VXDeYh9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "98f6a24f-a2d7-4280-badb-81107053180b"
      },
      "source": [
        "Question_dataset_train[0][1]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxURTJQzd0t1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data = model_gen(Question_dataset_train[0][0].unsqueeze(0).cuda(), Question_dataset_train[0][1].unsqueeze(0).cuda())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKTK1XqJeuRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a6b0a208-338a-4d45-c7a4-a3229fb102c0"
      },
      "source": [
        "print(tokenizer.convert_ids_to_tokens(torch.argmax(data[0][0], dim=-1)))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['##ожд', '##апад', '##апад', '##апад', 'Терек', 'There', 'ɲ', 'Детали', '##ga', 'tem', 'Cher', 'Предприят', 'Предприят', 'Предприят', 'Sem', '##емар', '##емар', 'Sem', '##емар', 'Терек', 'Терек', 'Терек', 'Siemens', 'лотов', '##yx', '##енья', 'Предприят', '##yx', 'литературному', 'литературному', 'оперативных', '##емар', 'становились', 'становились', 'становились', '##апад', 'Sem', 'Терек', 'Терек', 'любили', 'Siemens', 'Терек', '##yx', '##енья', '##енья', '##апад', '##апад', '##апад', '##апад', 'становились', '##апад', 'становились', 'Терек', 'Терек', 'Терек', 'Терек', 'любили', 'любили', 'Терек', 'Предприят', '##yx', '##енья', 'Дмитриевич', 'Луз', 'Комиссии', '##емар', 'становились', 'становились', '##апад', 'становились', 'Терек', 'летию', 'любили', '##ways', 'Терек', '##yx', '##енья', 'Предприят', '##yx', 'Комиссии', 'соглашениям', 'перестрелку', 'перестрелку', '##емар', '##емар', 'перестрелку', 'ŵ', 'ŵ', '##ways', 'летию', 'госбезопасности', 'Терек', 'Предприят', '##yx', 'Предприят', 'Чел', 'Комиссии', 'Комиссии', 'перестрелку', '##емар', '##емар', 'становились', 'становились', 'Терек', 'Высоцкого', 'Терек', '##ways', '##лара', 'Терек', '##улир', 'Предприят', 'Предприят', 'Предприят', 'Комиссии', 'Комиссии', 'перестрелку', '##емар', '##емар', 'становились', 'становились', 'ŵ', 'Терек', 'летию', 'госбезопасности', '##раг', 'IGN', '##yx', 'Micros', 'Предприят', 'Предприят', 'Комиссии', 'соглашениям', 'перестрелку', 'перестрелку', '##ожд', '##ожд', '##ожд', '##ожд', '##ожд', '##ожд', 'Терек', 'госбезопасности', 'Терек', 'Терек', 'Предприят', '##yx', 'Предприят', 'Предприят', 'Предприят', 'Комиссии', 'Комиссии', 'перестрелку', 'перестрелку', 'перестрелку', 'Андрес', '##ожд', 'Андрес', '##ожд', '##ожд', 'Андрес', 'Терек', 'Терек', 'Комиссии', 'Предприят', 'Предприят', 'Комиссии', 'Комиссии', 'перестрелку', 'перестрелку', 'перестрелку', 'госбезопасности', '##ожд', '##ожд', '##ожд', '##цеп', 'Андрес', 'госбезопасности', '##ожд', 'Терек', 'Предприят', '##yx', 'Предприят', 'Комиссии', 'Комиссии', 'перестрелку', 'перестрелку', 'перестрелку', 'перестрелку', '##ожд', 'госбезопасности', 'перестрелку', '##ожд', '##ожд', '##цеп', '##емар', 'госбезопасности', 'госбезопасности', 'Терек', 'Терек', '##ъезд', '##yx', 'Предприят', 'Предприят', 'Micros', 'Предприят', 'Комиссии', 'литературному', 'перестрелку', 'перестрелку', 'перестрелку', '##ожд', '##ожд', '##ожд', 'гра', 'госбезопасности', '##ожд', 'Терек', 'госбезопасности', 'Терек', 'Терек', 'Предприят', '##yx', 'Micros', 'Предприят', 'Предприят', 'Комиссии', 'Комиссии', 'Комиссии', 'соглашениям', 'перестрелку', 'перестрелку', 'госбезопасности', 'перестрелку', 'госбезопасности', 'госбезопасности', 'госбезопасности', '##ожд', 'Андрес', '##ожд', '##емар', 'Терек', '##лара', 'любили', 'Терек', 'IGN', '##yx', 'Micros', 'Предприят', '##yx', 'перестрелку', 'литературному', 'литературному', 'оперативных', '##емар', 'становились', '##апад', 'становились', '##апад', '##апад', '##апад', '##апад', 'Sem', 'Терек', 'летию', 'любили', '##ways', 'There', 'любили', 'радаров', 'синтеза', '##yx', '##илог', 'Предприят', '##илог', 'Предприят', 'Чел', 'литературному', 'соглашениям', 'перестрелку', 'госбезопасности', '##емар', 'госбезопасности', 'госбезопасности', '##емар', '##емар', 'Терек', 'Терек', 'Терек', 'Терек', '##ways', '##лара', '##ways', 'Терек', 'IGN', '##yx', '##yx', 'Предприят', 'Предприят', 'Комиссии', 'Комиссии', 'Комиссии', 'Комиссии', 'перестрелку', 'перестрелку', 'перестрелку', 'госбезопасности', '##ожд', '##ожд', '##ожд', '##цеп', '##емар', 'госбезопасности', 'госбезопасности', 'Терек', 'Терек', '##yx', '##yx', '##енья', 'Предприят', '##yx', 'Предприят', 'Предприят', 'Дмитриевич', 'Комиссии', 'перестрелку', 'перестрелку', 'перестрелку', 'перестрелку', '##ожд', '##ожд', '##ожд', '##ожд', '##ожд', '##ожд', '##ожд', 'Андрес', 'госбезопасности', '##ожд', 'Терек', 'Терек', 'Терек', 'Терек', 'Терек', 'Предприят', '嬉', '##yx', 'Предприят', 'Комиссии', 'Комиссии', 'Комиссии', 'Комиссии', 'перестрелку', 'перестрелку', 'перестрелку', 'перестрелку', 'перестрелку', 'перестрелку', '##ожд', 'перестрелку', '##ожд', 'гра', 'гра', '##ожд', '##ожд', 'Андрес', 'госбезопасности', '##ожд']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}