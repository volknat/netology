{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "92RgJ4kqmKtL",
        "colab_type": "code",
        "outputId": "ba3da4c4-1be2-4244-cb71-c11afcc9ab25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy7q043LlYtH",
        "colab_type": "code",
        "outputId": "e5bf4693-7c63-4acb-cb12-3c23fc2425da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import BertModel, AdamW, BertConfig, BertTokenizer, Model2Model, PreTrainedEncoderDecoder, BertPreTrainedModel\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import re\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeRs4UaslgZm",
        "colab_type": "code",
        "outputId": "d199c20d-ea99-4ba7-b80a-90ae8531f9e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZqs_QQ3lYtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config_path = '/content/drive/My Drive/Rubert/rubert_cased_L-12_H-768_A-12_pt/bert_config.json'\n",
        "checkpoint_path = '/content/drive/My Drive/Rubert/rubert_cased_L-12_H-768_A-12_pt/pytorch_model.bin'\n",
        "vocab_path = '/content/drive/My Drive/Rubert/rubert_cased_L-12_H-768_A-12_pt/vocab.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HlPoFqdlYtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_dict = {'negative':0, 'neutral': 1, 'positive':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0hx3wjqlYtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, path, label_dict, suftrain=True):\n",
        "        self.df = pd.read_json(path)\n",
        "        X_train, X_test = train_test_split(self.df, test_size=0.2, random_state=42)\n",
        "        if suftrain == True:\n",
        "          self.df = X_train.reset_index(drop=True)\n",
        "        else:\n",
        "          self.df = X_test.reset_index(drop=True)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(vocab_path, do_lower_case=True)\n",
        "        self.label_dict = label_dict\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.df.loc[index, 'text']\n",
        "        label = label_dict[self.df.loc[index, 'sentiment']]\n",
        "        tokens = self.tokenizer.tokenize(sentence)\n",
        "        \n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "        \n",
        "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        tokens_ids_tensor = torch.tensor(tokens_ids)\n",
        "       \n",
        "        return tokens_ids_tensor,  label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReuyniDflYtu",
        "colab_type": "code",
        "outputId": "0b86819f-e01e-4f45-eaea-83ba3c914c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_dataset = SentimentDataset('/content/drive/My Drive/sentiment/train.json', label_dict)\n",
        "test_dataset = SentimentDataset('/content/drive/My Drive/sentiment/train.json', label_dict, suftrain=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3Vjl-yKeeCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sequences(x, max_len):\n",
        "    if len(x) <max_len:\n",
        "        padded = list(x.numpy()) + [0]*(max_len - len(x))\n",
        "    else:\n",
        "        padded = list(x.numpy())[:(max_len-1)] + [102]\n",
        "    return padded    \n",
        "\n",
        "\n",
        "class Collator(object):\n",
        "    def __init__(self, percentile=100):\n",
        "        self.percentile = percentile\n",
        "        \n",
        "    def __call__(self, batch):\n",
        "\n",
        "        \n",
        "        #inp, targ, inp3, lens, lens_targ\n",
        "        \n",
        "        inp, targ = zip(*batch)\n",
        "        \n",
        "        lens = [len(x) for x in inp] \n",
        "        max_len = int(np.percentile(lens, self.percentile))\n",
        "        if max_len > 400:\n",
        "          max_len=400\n",
        "        inp = torch.from_numpy(np.array([pad_sequences(sentence, max_len) for sentence in inp]))\n",
        "        \n",
        "        targ = torch.tensor(targ)\n",
        "        \n",
        "        return inp, targ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEdned05lYt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#settings\n",
        "batch_size = 12\n",
        "epochs = 3\n",
        "learning_rate = 0.00002"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX4nWEJrh4yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collate = Collator(percentile=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lCkfnmtlYt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collate)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUwViQ8glYt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelRusNewsSentiment(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelRusNewsSentiment, self).__init__()\n",
        "        \n",
        "        self.Rubert = BertModel.from_pretrained(checkpoint_path, config=config_path) \n",
        "        self.Dropout = nn.Dropout(0.1)\n",
        "        self.FC = nn.Linear(768, 3)\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self,seq):\n",
        "        attn_mask = (seq != 0).long()\n",
        "        out, _ = self.Rubert(seq, attention_mask = attn_mask)\n",
        "        cls = out[:, 0]\n",
        "        logits = self.FC(cls)\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD8mirhPlYuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ModelRusNewsSentiment()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or-ZU_7RlYuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = nn.CrossEntropyLoss(reduction = 'sum')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptteaBemlYuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, loss, test_loader):\n",
        "    losses, accuraces = 0.0, 0.0\n",
        "    model.eval()\n",
        "    \n",
        "    quantity = 0\n",
        "\n",
        "\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for (seq, label) in test_loader:\n",
        "          \n",
        "          quantity += len(seq)\n",
        "          seq, label = seq.cuda(), label.cuda()\n",
        "          val_logits = model (seq)\n",
        "          val_loss = loss(val_logits, label)\n",
        "          losses += val_loss\n",
        "          pred = torch.argmax(val_logits, dim=1)\n",
        "          accuraces += sum((pred == label)*1)\n",
        "         \n",
        "    return losses/quantity, accuraces/quantity\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a_Z7b6klYuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, test_loader,  epochs, loss):\n",
        "    model.cuda()\n",
        "    model.train()\n",
        " \n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.3)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "    \n",
        "        total_loss = 0.0\n",
        "        accuracy = 0.0\n",
        "        quantity = 0.0\n",
        "        for  (seq,  label) in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "       \n",
        "            quantity += len(seq)\n",
        "            seq, label = seq.cuda(), label.cuda()          \n",
        "\n",
        "            logits = model(seq)\n",
        "\n",
        "            train_loss = loss(logits, label)\n",
        "            train_loss.backward()\n",
        "            total_loss = total_loss + train_loss\n",
        "            \n",
        "            pred = torch.argmax(logits, dim=1)\n",
        "            accuracy += sum((pred == label)*1)\n",
        "            \n",
        "\n",
        "            nn.utils.clip_grad_norm_(model.parameters(),0.5)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        val_loss, val_acc = evaluate(model, loss, test_loader)\n",
        "        scheduler.step()\n",
        "        print(f'Epoch {epoch}, Train_loss: {total_loss/quantity},Train_accuracy: {accuracy/quantity}')\n",
        "        print(f'Epoch {epoch}, Val_loss: {val_loss},Val_accuracy: {val_acc}')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMHD-_gglYua",
        "colab_type": "code",
        "outputId": "d81235ae-dbe1-44d7-ca23-5c1a65704807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "model = train(model, train_loader, test_loader,  epochs, loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Train_loss: 0.8048166632652283,Train_accuracy: 0.6127272844314575\n",
            "Epoch 0, Val_loss: 0.7122477293014526,Val_accuracy: 0.6745741963386536\n",
            "Epoch 1, Train_loss: 0.5657979846000671,Train_accuracy: 0.7478787899017334\n",
            "Epoch 1, Val_loss: 0.6435496807098389,Val_accuracy: 0.7165449857711792\n",
            "Epoch 2, Train_loss: 0.41886213421821594,Train_accuracy: 0.8298484683036804\n",
            "Epoch 2, Val_loss: 0.6916881799697876,Val_accuracy: 0.7080292105674744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqo3YzrWlYuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}