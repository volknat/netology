{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of l05c03_exercise_flowers_with_data_augmentation.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l05c03_exercise_flowers_with_data_augmentation.ipynb","timestamp":1563422449460}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TBFXQGKYUc4X"},"source":["##### Copyright 2018 The TensorFlow Authors."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FE7KNzPPVrVV"},"source":["# Flower Classification using tf.keras"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rtPGh2MAVrVa","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import os\n","import numpy as np\n","import glob\n","import shutil\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"L1WtoaOHVrVh","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UZZI6lNkVrVm"},"source":["# Data Loading"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OYmOylPlVrVt","colab":{}},"source":["_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n","\n","zip_file = tf.keras.utils.get_file(origin=_URL, \n","                                   fname=\"flower_photos.tgz\", \n","                                   extract=True)\n","\n","base_dir = os.path.join(os.path.dirname(zip_file), 'flower_photos')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2yge5MKnnjMd"},"source":["The  dataset we downloaded contains images of 5 types of flowers:\n","\n","1. Rose\n","2. Daisy\n","3. Dandelion\n","4. Sunflowers\n","5. Tulips\n","\n","So, let's create the labels for these 5 classes: "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FiYVs1MEmNHf","colab":{}},"source":["classes = ['roses', 'daisy', 'dandelion', 'sunflowers', 'tulips']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uh68rmWspp0U","colab":{}},"source":["train_dir = os.path.join(base_dir, 'train')\n","val_dir = os.path.join(base_dir, 'val')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UOoVpxFwVrWy"},"source":["# Data Augmentation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QyPkET61yMMX","colab":{}},"source":["batch_size =100\n","IMG_SHAPE = 150"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Bi1_vHyBVrW2","colab":{}},"source":["image_gen_train = ImageDataGenerator(rescale=1./255, rotation_range=30, zoom_range=[0.3,0.3], horizontal_flip=True)\n","\n","train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size, \n","                                                     directory=train_dir, \n","                                                     shuffle=True, \n","                                                     target_size=(IMG_SHAPE,IMG_SHAPE))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"a99fDBt7VrXr"},"source":["### TODO: Create a Data Generator for the Validation Set\n","\n","Generally, we only apply data augmentation to our training examples. So, in the cell below, use ImageDataGenerator to create a transformation that only rescales the images by 255. Then use the `.flow_from_directory` method to apply the above transformation to the images in our validation set. Make sure you indicate the batch size, the path to the directory of the validation images, the target size for the images, and to set the class mode to `sparse`. Remember that it is not necessary to shuffle the images in the validation set. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"54x0aNbKVrXr","colab":{}},"source":["image_gen_val = ImageDataGenerator(rescale=1./255)\n","val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size, \n","                                                     directory=train_dir, \n","                                                     shuffle=True, \n","                                                     target_size=(IMG_SHAPE,IMG_SHAPE))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wEgW4i18VrWZ"},"source":["# TODO: Create the CNN\n","\n","In the cell below, create a convolutional neural network that consists of 3 convolution blocks. Each convolutional block contains a `Conv2D` layer followed by a max pool layer.  The first convolutional block should have 16 filters, the second one should have 32 filters, and the third one should have 64 filters. All convolutional filters should be 3 x 3. All max pool layers should have a `pool_size` of `(2, 2)` . \n","\n","After the 3 convolutional blocks you should have a flatten layer followed by a fully connected layer with 512 units. The CNN should output class probabilities based on 5 classes which is done by the **softmax** activation function. All other layers should use a **relu** activation function. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Evjf8jZk2zi-","colab":{}},"source":["model = model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(64, (3,3), input_shape=(150, 150, 3)),\n","                                            tf.keras.layers.BatchNormalization(),\n","                                            tf.keras.layers.LeakyReLU(),\n","                                            tf.keras.layers.MaxPooling2D(2, 2),\n","                                            tf.keras.layers.Dropout(0.2),\n","                                            tf.keras.layers.Conv2D(32, (3,3), input_shape=(150, 150, 3)),\n","                                            tf.keras.layers.BatchNormalization(),\n","                                            tf.keras.layers.LeakyReLU(),\n","                                            tf.keras.layers.MaxPooling2D(2, 2),\n","                                            tf.keras.layers.Dropout(0.2),\n","                                            tf.keras.layers.Conv2D(16, (3,3), input_shape=(150, 150, 3)),\n","                                            tf.keras.layers.BatchNormalization(),\n","                                            tf.keras.layers.LeakyReLU(),\n","                                            tf.keras.layers.MaxPooling2D(2, 2),\n","                                            tf.keras.layers.Dropout(0.2),\n","                                       \n","                                            tf.keras.layers.Flatten(),\n","                                            tf.keras.layers.Dense(512, activation='relu'),\n","                                            tf.keras.layers.Dropout(0.5),\n","                                            tf.keras.layers.Dense(256, activation='relu'),\n","                                            tf.keras.layers.Dropout(0.5),\n","                                            tf.keras.layers.Dense(5, activation='sigmoid')\n","                                            ])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DADWLqMSJcH3"},"source":["# TODO: Compile the Model\n","\n","In the cell below, compile your model using the ADAM optimizer, the sparse cross entropy function as a loss function. We would also like to look at training and validation accuracy on each epoch as we train our network, so make sure you also pass the metrics argument."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"08rRJ0sn3Tb1","colab":{}},"source":["optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n","model.compile(optimizer = optimizer, loss='binary_crossentropy', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vhoT6DdH5hTl","colab_type":"code","colab":{}},"source":["total_train =sum([len(os.listdir(os.path.join(train_dir, x))) for x in os.listdir(train_dir)])\n","total_val = sum([len(os.listdir(os.path.join(val_dir, x))) for x in os.listdir(val_dir)])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oub9RtoFVrWk"},"source":["# TODO: Train the Model\n","\n","In the cell below, train your model using the  **fit_generator** function instead of the usual **fit** function. We have to use the `fit_generator` function because we are using the **ImageDataGenerator** class to generate batches of training and validation data for our model. Train the model for 80 epochs and make sure you use the proper parameters in the `fit_generator` function . "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tk5NT1PW3j_P","colab":{}},"source":["epochs = 10\n","\n","history = model.fit_generator(\n","    train_data_gen,\n","    steps_per_epoch=int(np.ceil(total_train / float(batch_size))),\n","    epochs=epochs,\n","    validation_data=val_data_gen,\n","    validation_steps=int(np.ceil(total_val / float(batch_size))),\n","    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss')]\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LZPYT-EmVrWo"},"source":["# TODO: Plot Training and Validation Graphs.\n","\n","In the cell below, plot the training and validation accuracy/loss graphs."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8CfngybnFHQR","colab":{}},"source":["acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(7)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()\n","\n"],"execution_count":0,"outputs":[]}]}