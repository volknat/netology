{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hrbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id_UT3HX-wBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !export IMAGE_FAMILY=\"pytorch-latest-gpu\"\n",
        "# !export ZONE=\"us-west1-b\"\n",
        "# !export INSTANCE_NAME=\"pytorch-colab-backend\"\n",
        "# !gcloud compute ssh --zone ZONE INSTANCE_NAME -- -L 8888:localhost:8888"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFXa8SHrXzan",
        "colab_type": "code",
        "outputId": "40b037ae-d09d-42af-93e2-72e3a3dba670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "!pip install transformers==2.5.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.5.0 in /usr/local/lib/python3.6/dist-packages (2.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (1.17.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (0.0.38)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (0.1.85)\n",
            "Requirement already satisfied: tokenizers==0.5.0 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (0.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (1.11.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.0) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.0) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.0) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.0) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.0) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.0) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.0) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.0) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.0) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.5.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.5.0) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOF2qFLicBj9",
        "colab_type": "code",
        "outputId": "042392f7-a18b-45b8-f2f2-952efc829470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import BertModel, AdamW, BertConfig, BertTokenizer, Model2Model, PreTrainedEncoderDecoder, BertPreTrainedModel\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import re\n",
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQic_VP_bwmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#HRbot w/o end and start answer tokens. Answer parts are located in different context sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvGnU1M3uqes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLTqCCpn_WrP",
        "colab_type": "code",
        "outputId": "300a1182-921c-4699-f462-15d213097cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "data = pd.read_csv('drive/My Drive/hrbot.csv')\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Какое управленческое действие относится к функ...</td>\n",
              "      <td>планирование, прогнозирование, мотивация, орга...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Управленческий персонал включает:</td>\n",
              "      <td>руководителей, специалистов</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>К функциям менеджмента относят</td>\n",
              "      <td>планирование, прогнозирование, мотивация, орга...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>К японскому менеджменту персонала относится:</td>\n",
              "      <td>продвижение  зависит от возраста рабочего или ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>С какими дисциплинами связана система наук о т...</td>\n",
              "      <td>экономика труда, психология труда, физиология ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question                                             Answer\n",
              "0  Какое управленческое действие относится к функ...  планирование, прогнозирование, мотивация, орга...\n",
              "1                  Управленческий персонал включает:                        руководителей, специалистов\n",
              "2                     К функциям менеджмента относят  планирование, прогнозирование, мотивация, орга...\n",
              "3       К японскому менеджменту персонала относится:  продвижение  зависит от возраста рабочего или ...\n",
              "4  С какими дисциплинами связана система наук о т...  экономика труда, психология труда, физиология ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0TYobWuj9YS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data tokenizetion for bert\n",
        "class BotDataset(Dataset):\n",
        "  def __init__(self, data, column, context_atn=False,max_leninf =False):\n",
        "    self.df = data[column].copy()\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
        "    if max_leninf == False:\n",
        "      self.maxlen =  max(data[column].apply(lambda x:len(x)))\n",
        "    else:\n",
        "      self.maxlen = max_leninf\n",
        "    self.context_atn = context_atn\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sentence = self.df.loc[index]\n",
        "    tokens = self.tokenizer.tokenize(sentence)\n",
        "    if self.context_atn == False:\n",
        "      tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "    if len(tokens) < self.maxlen:\n",
        "      tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]\n",
        "    else:\n",
        "      if self.context_atn == False:\n",
        "        tokens = tokens[:(self.maxlen-1)] + ['[SEP]']\n",
        "      else: \n",
        "        tokens = tokens[:self.maxlen]\n",
        "    tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "    tokens_ids_tensor = torch.tensor(tokens_ids)\n",
        "    attn_mask = (tokens_ids_tensor != 0).long()\n",
        "    return tokens_ids_tensor, attn_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldtQGH1iKWWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_len = len(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ulJekvZBOJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "border = int(0.8*data_len)\n",
        "Question_dataset_train = BotDataset(data.iloc[:border], 'Question')\n",
        "Question_dataset_test = BotDataset(data.iloc[border:], 'Question')\n",
        "Answer_dataset_train = BotDataset(data.iloc[:border], 'Answer')\n",
        "Answer_dataset_test = BotDataset(data.iloc[border:], 'Answer')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYeNgI6CMi0M",
        "colab_type": "code",
        "outputId": "006a0451-a8e4-447a-eb13-596eb659210f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "#Context = hr management issues database\n",
        "context = open('drive/My Drive/context.txt', 'r').read()\n",
        "print(len(context))\n",
        "print(context[:30])\n",
        "pattern = re.compile('[^А-Яа-яЁёA-Za-z0-9/./,/:/?/]')\n",
        "context = pattern.sub(' ', context)\n",
        "context = pd.DataFrame(context.split('.'), columns = ['sentences'])\n",
        "context_len = len(context)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "796689\n",
            "﻿УПРАВЛЕНИЕ ПЕРСОНАЛОМ\n",
            "Под ред\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF9rkCRXV7t1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# context for choosing appropriate sentences for attention \n",
        "context_dataset = BotDataset(context, 'sentences', False,16)\n",
        "\n",
        "# context for answer generation\n",
        "context_dataset_answgen = BotDataset(context, 'sentences', True,16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WERs-x-UBHJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get CLS embedding \n",
        "model_qc = BertModel.from_pretrained('bert-base-multilingual-uncased').to(device)\n",
        "model_qc.eval()\n",
        "\n",
        "#question\n",
        "q_tensor_train = torch.stack([Question_dataset_train[x][0] for x in range(border)]).to(device)\n",
        "q_tensor_test = torch.stack([Question_dataset_test[x][0] for x in range(border, data_len)]).to(device)\n",
        "q_atn_train = torch.stack([Question_dataset_train[x][1] for x in range(border)]).to(device)\n",
        "q_atn_test = torch.stack([Question_dataset_test[x][1] for x in range(border, data_len)]).to(device)\n",
        "with torch.no_grad():\n",
        "  q_hid_train, _ = model_qc(q_tensor_train, q_atn_train)\n",
        "  q_hid_test, _ = model_qc(q_tensor_test, q_atn_test)\n",
        "q_hid_train = q_hid_train[:, 0]\n",
        "q_hid_test = q_hid_test[:, 0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0E1mYfQ9qI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_tensor_train = torch.stack([Answer_dataset_train[x][0] for x in range(border)]).to(device)\n",
        "a_tensor_test = torch.stack([Answer_dataset_test[x][0] for x in range(border, data_len)]).to(device)\n",
        "\n",
        "a_tensor_atn_train = torch.stack([Answer_dataset_train[x][1] for x in range(border)]).to(device)\n",
        "a_tensor_atn_test = torch.stack([Answer_dataset_test[x][1] for x in range(border, data_len)]).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Uz3oFcOCuih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(q_hid_train, a_tensor_train, a_tensor_atn_train)\n",
        "test_dataset = torch.utils.data.TensorDataset(q_hid_test, a_tensor_test, a_tensor_atn_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2gAZdEW7upo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#context for choosing sentence\n",
        "c_tensor = torch.stack([context_dataset[x][0] for x in range(context_len)]).to(device)\n",
        "c_atn = torch.stack([context_dataset[x][1] for x in range(context_len)]).to(device)\n",
        "with torch.no_grad():\n",
        "  c_hid, _ = model_qc(c_tensor, c_atn)\n",
        "  c_hid = c_hid[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvnwYJKHhaUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#context for text generation\n",
        "c_tensor = torch.stack([context_dataset_answgen[x][0] for x in range(context_len)]).to(device)\n",
        "c_atn = torch.stack([context_dataset_answgen[x][1] for x in range(context_len)]).to(device)\n",
        "with torch.no_grad():\n",
        "  w_embed, _ = model_qc(c_tensor, c_atn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD9PybaFoOvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model_qc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mzkk4HGeXlK",
        "colab_type": "code",
        "outputId": "d0fcb308-79cb-4f62-d48a-cb561b8160f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "c_tensor[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([28069, 32623, 40705, 21015, 11484, 77377, 76854,   324,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiP5_FgHmXbV",
        "colab_type": "code",
        "outputId": "d0c8093e-4dfc-4331-f5a7-c1ce4061eb32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "q_hid_train.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([544, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E7erfm2h-5V",
        "colab_type": "code",
        "outputId": "e98ff139-6489-40ca-b505-eeefd48502e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "w_embed.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6009, 16, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKcJCM6bnEnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#settings\n",
        "batch_size = 4 #max possible\n",
        "embedding_dim = q_hid_train.size()[1]\n",
        "attn_size = 16\n",
        "learning_rate = 0.00002"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg1XPBRLXToo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size, shuffle=False, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BkEWWWfMRLp",
        "colab_type": "code",
        "outputId": "89095a80-bfcc-4391-edbc-55da0b1d7462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
        "tokenizer.sep_token_id"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DIBrsmVDrh6J",
        "colab": {}
      },
      "source": [
        "# Source: https://github.com/huggingface/transformers/pull/1455/files  p.s. WIP code, so changes are:  mistakes fixed, prob list added\n",
        "softm = torch.nn.Softmax(dim=1)\n",
        "class TransformerBeamSearch(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        vocab_size,\n",
        "        tokenizer,\n",
        "        batch_size,\n",
        "        beam_size,\n",
        "        min_length,\n",
        "        max_length,\n",
        "        alpha=0,\n",
        "        block_repeating_trigram=True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Attributes:\n",
        "            mask_word_id: token id that corresponds to the mask\n",
        "        \"\"\"\n",
        "        super(TransformerBeamSearch, self).__init__()\n",
        "        self.model = model\n",
        "        decoder_config = transformers.AutoConfig.from_pretrained('bert-base-multilingual-uncased', is_decoder=True)\n",
        "        self.end_token_id = tokenizer.sep_token_id\n",
        "        self.start_token_id = tokenizer.cls_token_id\n",
        "        self.beam_size = beam_size\n",
        "        self.min_length = min_length\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.block_repeating_trigram = block_repeating_trigram\n",
        "        self.apply_length_penalty = False if alpha == 0 else True\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # State of the beam\n",
        "        self.hypotheses = [[] for _ in range(batch_size)]\n",
        "        self.batch_offset = torch.arange(batch_size, dtype=torch.long).cuda()\n",
        "        self.beam_offset = torch.arange(\n",
        "            0, batch_size * self.beam_size, step=self.beam_size, dtype=torch.long\n",
        "        ).cuda()\n",
        "        self.growing_beam = torch.full(\n",
        "            (batch_size * self.beam_size, 1), self.start_token_id, dtype=torch.long\n",
        "        )\n",
        "        self.growing_prob = torch.full(\n",
        "            (batch_size,1,self.vocab_size), 0, dtype=torch.long\n",
        "        ).cuda()\n",
        "\n",
        "        self.topk_log_probabilities = torch.tensor(\n",
        "            [0.0] + [float(\"-inf\")] * (self.beam_size - 1), dtype=torch.float\n",
        "        ).repeat(batch_size)\n",
        "        self.results = {\n",
        "            \"predictions\": [[] for _ in range(batch_size)],\n",
        "            \"scores\": [[] for _ in range(batch_size)],\n",
        "            \"probs\": [[] for _ in range(batch_size)],\n",
        "        }\n",
        "        self._step = 0\n",
        "        self.is_done = False\n",
        "\n",
        "    def step(self, log_probabilities):\n",
        "        \"\"\" Grows the beam by one step. \"\"\"\n",
        "        self._step += 1\n",
        "\n",
        "        # The batch size changes as some beams finish so we define _B\n",
        "        vocab_size = log_probabilities.size(-1)\n",
        "        _B = log_probabilities.size(0) // self.beam_size\n",
        "\n",
        "        # Multiply each beam probability with the probability of the\n",
        "        # next token (conditioned on the words in the beam).\n",
        "      \n",
        "\n",
        "        log_probabilities = log_probabilities.squeeze(1)\n",
        "        \n",
        "        log_probabilities += self.topk_log_probabilities.view(-1, 1).cuda()\n",
        "\n",
        "        log_probabilities = self.enforce_min_length(log_probabilities)\n",
        "        if self.block_repeating_trigram:\n",
        "            self.remove_repeating_trigrams(log_probabilities, _B)\n",
        "\n",
        "       \n",
        "        # Find the `beam_size` (previous_beam + token) combinations with\n",
        "        # the highest score\n",
        "        topk_log_probabilities, topk_ids = torch.topk(\n",
        "            log_probabilities.view(_B, self.beam_size * vocab_size),\n",
        "            self.beam_size,\n",
        "            dim=1) \n",
        "        prob = log_probabilities.view(_B, -1, vocab_size)\n",
        "        prob = torch.max(prob, dim=1).values.unsqueeze(1).long()\n",
        "        prob = (prob >= torch.min(topk_log_probabilities, dim=1).values.unsqueeze(1))*1\n",
        "        \n",
        "     \n",
        "        \n",
        "\n",
        "        # Apply the length penalty. The +1 accounts for the [EOS] token\n",
        "        # that will be added if the beam ends.\n",
        "        topk_scores = topk_log_probabilities / self.length_penalty()\n",
        "\n",
        "        # Retrieve the corresponding respective beam and token id\n",
        "        # topk_token_ids[i] will be added to topk_beam_ids[i]\n",
        "        topk_beam_ids = topk_ids.div(vocab_size)\n",
        "        topk_token_ids = topk_ids.fmod(vocab_size)\n",
        "\n",
        "        # Retrieve the row index of the surviving beams in the original\n",
        "        # view of the log_probabilities tensor\n",
        "        surviving_beams_rows = (topk_beam_ids + self.beam_offset[:_B].view(-1, 1).cuda()).view(\n",
        "            -1\n",
        "        )\n",
        "\n",
        "        # Append the last predictions\n",
        "        self.growing_beam = torch.cat(\n",
        "            [\n",
        "                self.growing_beam.index_select(0, surviving_beams_rows),\n",
        "                topk_token_ids.view(-1, 1),\n",
        "            ],\n",
        "            1,\n",
        "        )\n",
        "        self.growing_prob = torch.cat(\n",
        "            [\n",
        "                self.growing_prob,\n",
        "                prob.cuda(),\n",
        "            ],\n",
        "            1,\n",
        "        )\n",
        "\n",
        "\n",
        "        # Check if any of the beam searches has ended during this\n",
        "        # growth step. Also if top beam (most probable) has ended\n",
        "        # for one element of the batch.\n",
        "        is_finished = topk_token_ids.eq(self.end_token_id)\n",
        "        is_finished = self.enforce_max_length(is_finished)\n",
        "        is_top_beam_finished = is_finished[:, 0].eq(1)\n",
        "\n",
        "        # Save the finished searches\n",
        "        if is_finished.any():\n",
        "            predictions = self.growing_beam.view(\n",
        "                -1, self.beam_size, self.growing_beam.size(1)\n",
        "            )\n",
        "            for i in range(is_finished.size(0)):\n",
        "                if is_top_beam_finished[i]:\n",
        "                    is_finished[i].fill_(1)\n",
        "                finished_hyp = is_finished[i].nonzero().view(-1)\n",
        "\n",
        "                # Store finished hypotheses for this batch.\n",
        "                b = self.batch_offset[i]\n",
        "                for j in finished_hyp:\n",
        "                    self.hypotheses[b].append((topk_scores[i, j], predictions[i, j, :]))\n",
        "\n",
        "                # If the batch reached the end, save the best hypotheses\n",
        "                # in terms of length-penalized score.\n",
        "                if is_top_beam_finished[i]:\n",
        "                    best_hyp = sorted(\n",
        "                        self.hypotheses[b], key=lambda x: x[0], reverse=True\n",
        "                    )\n",
        "                    best_score, best_prediction = best_hyp[0]\n",
        "                    self.results[\"scores\"][b].append(best_score)\n",
        "                    self.results[\"predictions\"][b].append(best_prediction)\n",
        "                    self.results[\"probs\"][b].append(self.growing_prob[b])\n",
        "\n",
        "            non_finished = is_top_beam_finished.eq(0).nonzero().view(-1).cuda()\n",
        "            if len(non_finished) == 0:\n",
        "                self.is_done = True\n",
        "\n",
        "            # Remove finished batches for the next step.\n",
        "            topk_log_probabilities = topk_log_probabilities.index_select(\n",
        "                0, non_finished\n",
        "            )\n",
        "            self.batch_offset = self.batch_offset.index_select(0, non_finished)\n",
        "            self.growing_beam = predictions.index_select(0, non_finished).view(\n",
        "                -1, self.growing_beam.size(-1)\n",
        "            )\n",
        "\n",
        "            surviving_beams_rows = surviving_beams_rows.index_select(0, non_finished)\n",
        "\n",
        "        return surviving_beams_rows\n",
        "\n",
        "    def forward(self, encoder_input_ids,encoder_attention_mask, **kwargs):\n",
        "        # keyword arguments come in 3 flavors: encoder-specific (prefixed by\n",
        "        # `encoder_`), decoder-specific (prefixed by `decoder_`) and those\n",
        "        # that apply to the model as whole.\n",
        "        # We let the specific kwargs override the common ones in case of conflict.\n",
        "        kwargs_encoder = {\n",
        "            argument[len(\"encoder_\"):]: value\n",
        "            for argument, value in kwargs.items()\n",
        "            if argument.startswith(\"encoder_\")\n",
        "        }\n",
        "        kwargs_decoder = {\n",
        "            argument[len(\"decoder_\"):]: value\n",
        "            for argument, value in kwargs.items()\n",
        "            if argument.startswith(\"decoder_\")\n",
        "        }\n",
        "        kwargs_common = {\n",
        "            argument: value\n",
        "            for argument, value in kwargs.items()\n",
        "            if not (argument.startswith(\"encoder_\") or argument.startswith(\"decoder_\"))\n",
        "        }\n",
        "        kwargs_decoder = dict(kwargs_common, **kwargs_decoder)\n",
        "        kwargs_encoder = dict(kwargs_common, **kwargs_encoder)\n",
        "\n",
        "        # forward pass on the encoder\n",
        "        encoder_outputs, _ = self.model.encoder.forward(encoder_input_ids, encoder_attention_mask)\n",
        "\n",
        "        encoder_hidden_states = tile(\n",
        "            encoder_outputs, self.beam_size, dim=0\n",
        "        )\n",
        "\n",
        "        # grow the beam by generating sequences in an autoregressive way\n",
        "        self.growing_beam = torch.full(\n",
        "            (self.batch_size * self.beam_size, 1), self.start_token_id, dtype=torch.long).cuda()\n",
        "        for step in range(self.max_length):\n",
        "            decoder_input = self.growing_beam[:, -1]\n",
        "            outputs = self.model.decoder(decoder_input.view(-1,1).cuda(), encoder_hidden_states = encoder_hidden_states.cuda())\n",
        "            \n",
        "            log_probabilities = torch.nn.functional.log_softmax(outputs[0])\n",
        "            \n",
        "            surviving_beams_rows = self.step(log_probabilities)\n",
        "            if self.is_done:\n",
        "                break\n",
        "\n",
        "            encoder_hidden_states = encoder_hidden_states.index_select(0, surviving_beams_rows)\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def remove_repeating_trigrams(self, log_probabilities, _B):\n",
        "        if(self._step + 1 > 3):\n",
        "            for i in range(_B * self.beam_size):\n",
        "                tokens = [t for t in self.growing_beam[i]]\n",
        "                trigrams = [(tokens[i-1], tokens[i], tokens[i+1]) for i in range(1, len(tokens) - 1)]\n",
        "                last_trigram = tuple(trigrams[-1])\n",
        "                if last_trigram in trigrams[:-1]:\n",
        "                    log_probabilities[i] = -1e20\n",
        "\n",
        "    def enforce_min_length(self, log_probabilities):\n",
        "        if self._step < self.min_length:\n",
        "            log_probabilities[:, self.end_token_id] = -1e20\n",
        "        return log_probabilities\n",
        "\n",
        "    def enforce_max_length(self, is_finished):\n",
        "        if self._step + 1 == self.max_length:\n",
        "            is_finished.fill_(1)\n",
        "        return is_finished\n",
        "\n",
        "    def length_penalty(self):\n",
        "        return ((5.0 + (self._step + 1)) / 6.0) ** self.alpha\n",
        "\n",
        "\n",
        "def tile(x, count, dim=0):\n",
        "    \"\"\"\n",
        "    Tiles `x` along dimension `dim` `count` times.\n",
        "    Example:\n",
        "        >> ex = torch.tensor([1,2],[3,4])\n",
        "        >> tile(ex, 2, 0)\n",
        "        torch.Tensor([[1,2],[1,2],[3,4],[3,4]])\n",
        "    \"\"\"\n",
        "    perm = list(range(len(x.size())))\n",
        "    if dim != 0:\n",
        "        perm[0], perm[dim] = perm[dim], perm[0]\n",
        "        x = x.permute(perm).contiguous()\n",
        "    out_size = list(x.size())\n",
        "    out_size[0] *= count\n",
        "    batch = x.size(0)\n",
        "    x = (\n",
        "        x.view(batch, -1)\n",
        "        .transpose(0, 1)\n",
        "        .repeat(count, 1)\n",
        "        .transpose(0, 1)\n",
        "        .contiguous()\n",
        "        .view(*out_size)\n",
        "    )\n",
        "    if dim != 0:\n",
        "        x = x.permute(perm).contiguous()\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWWYfDzjaQ3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "class HRbotAnswerGen(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(HRbotAnswerGen, self).__init__()\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
        "    decoder_config = transformers.AutoConfig.from_pretrained('bert-base-multilingual-uncased', is_decoder=True)\n",
        "    self.model_gen = Model2Model.from_pretrained('bert-base-multilingual-uncased', decoder_config=decoder_config)\n",
        "    self.Q = nn.Linear(embedding_dim, attn_size)\n",
        "    torch.nn.init.xavier_uniform(self.Q.weight)\n",
        "    self.K = nn.Linear(embedding_dim, attn_size)\n",
        "    torch.nn.init.xavier_uniform(self.K.weight)\n",
        "    self.V = nn.Linear(embedding_dim, embedding_dim)\n",
        "    torch.nn.init.xavier_uniform(self.V.weight)\n",
        "    \n",
        "  def forward(self, w_embed, q_hid_batch, c_hid, c_tensor, c_atn, a_tensor = None, a_tensor_atn = None, test = False):\n",
        "    \n",
        "    q = 0\n",
        "    for question in q_hid_batch:\n",
        "      cos_q_tensor = question.repeat(c_hid.size(0),1)\n",
        "      cos_qc_tensor = cos(cos_q_tensor, c_hid)\n",
        "      _, attn_index = torch.sort(cos_qc_tensor, descending=True)\n",
        "      attn_index = attn_index[:10]\n",
        "      for index in attn_index:\n",
        "        w_embed_at = w_embed[index]\n",
        "        c_tensor_at = c_tensor[index]\n",
        "        # c_atn_at = c_atn[index]\n",
        "        if attn_index[0] == index:\n",
        "          w_embed_at_total = w_embed_at.clone()\n",
        "          c_tensor_at_total = c_tensor_at.clone()\n",
        "          # c_atn_at_total = c_atn_at.clone()\n",
        "        else:\n",
        "          w_embed_at_total = torch.cat([w_embed_at_total, w_embed_at], dim=0)\n",
        "          c_tensor_at_total = torch.cat([c_tensor_at_total, c_tensor_at], dim=0)\n",
        "          # c_atn_at_total = torch.cat([c_atn_at_total, c_atn_at], dim=0)\n",
        "      if q == 0:\n",
        "        question_total = w_embed_at_total.unsqueeze(0).clone()\n",
        "        question_context = c_tensor_at_total.unsqueeze(0).clone()\n",
        "        # question_c_atn = c_atn_at_total.unsqueeze(0).clone()\n",
        "        q = q + 1\n",
        "      else:\n",
        "        question_total = torch.cat([question_total, w_embed_at_total.unsqueeze(0)], dim=0)\n",
        "        question_context = torch.cat([question_context, c_tensor_at_total.unsqueeze(0)], dim=0)\n",
        "        # question_c_atn = torch.cat([question_c_atn, c_atn_at_total.unsqueeze(0)], dim=0)    \n",
        "\n",
        "    q_hid_batch = q_hid_batch.unsqueeze(1)\n",
        "    Q_question = self.Q(q_hid_batch)\n",
        "    K_wembed = self.K(question_total)\n",
        "    # V_embedgen = self.V(question_total)\n",
        "    softm_val = softm(torch.matmul(Q_question, K_wembed.permute(0,2,1))/(attn_size**(1/2)))\n",
        "    softm_val = softm_val.squeeze(1)\n",
        "\n",
        "\n",
        "    # --------for embedding input--------------------------------\n",
        "    # softm_matrix = torch.zeros(question_total.size(1),question_total.size(1)).repeat(batch_size,1,1).to(device)\n",
        "    # for i, _ in enumerate(softm_matrix):\n",
        "      # softm_matrix[i][torch.eye(question_total.size(1)).byte()] =softm_val[i]\n",
        "    # question_context_embed = torch.matmul(softm_matrix, V_embedgen)\n",
        "    \n",
        "\n",
        "\n",
        "    #----------for index input----------------------------------\n",
        "    softm_val = softm_val/torch.max(softm_val, dim=1).values.unsqueeze(1)\n",
        "    softm_val = torch.round(softm_val)\n",
        "    question_context = softm_val * question_context\n",
        "   \n",
        "    self.model_gen.encoder.requires_grad_=False\n",
        "    self.model_gen.decoder.requires_grad_=False\n",
        "    self.model_gen.decoder.cls.requires_grad_=True\n",
        "\n",
        "    \n",
        "    del cos_q_tensor\n",
        "    del cos_qc_tensor\n",
        "    del w_embed_at_total\n",
        "    del c_tensor_at_total\n",
        "    # del c_atn_at_total\n",
        "    del w_embed_at\n",
        "    del c_tensor_at\n",
        "    # del c_atn_at  \n",
        "    del question_total\n",
        "    # del question_c_atn\n",
        "   \n",
        "    question_context_atn = ((question_context == 0)*1).to(device)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    if test == False:\n",
        "      model_kwargs = {\"encoder_attention_mask\": question_context_atn, \"decoder_attention_mask\": a_tensor_atn }\n",
        "      out, _ , _= self.model_gen(encoder_input_ids=question_context.long(),decoder_input_ids=a_tensor.long(), **model_kwargs)\n",
        "      # model.encoder.layer[3].attention.self.key.weight.reqieres_grad = True\n",
        "      return out\n",
        "    else:\n",
        "      # model_kwargs = {\"encoder_attention_mask\": question_context_atn}\n",
        "      tran_beams = TransformerBeamSearch(self.model_gen, self.tokenizer.vocab_size,self.tokenizer, batch_size = q_hid_batch.size(0),beam_size=5, min_length = 205 ,max_length = 205)\n",
        "      out = tran_beams.forward(encoder_input_ids=question_context.long(), encoder_attention_mask = question_context_atn)\n",
        "      score = torch.stack([x[0] for x in out['scores']])\n",
        "      \n",
        "      prob = torch.stack([x[0] for x in out['probs']])\n",
        "   \n",
        "      pred = torch.stack([x[0] for x in out['predictions']])\n",
        "      return score, pred, prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u7HYJf49ddc",
        "colab_type": "code",
        "outputId": "4af974b3-26d6-4af5-8427-70c53c66aada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "model = HRbotAnswerGen()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewxo5tYf926d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_05Tz-Y-JwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
        "    total_loss = 0\n",
        "  \n",
        "    for i in range(len(real[0])):\n",
        "        \n",
        "      mask = real[:,i].ge(1).type(torch.cuda.FloatTensor)\n",
        "      crit = nn.CrossEntropyLoss()\n",
        "      loss_ = crit(pred[:,i].type(torch.cuda.FloatTensor), real[:,i].long()) * mask \n",
        "      total_loss  =  total_loss + torch.mean(loss_)\n",
        "    return total_loss/len(real[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRg-6bKDGV55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def evaluate(model, test_loader, device, c_hid, c_tensor, c_atn, w_embed):\n",
        "  model.eval()\n",
        "\n",
        "  for i, (q_hid_batch_test, answer_batch_test, answer_atn_batch_test) in enumerate(test_loader):\n",
        "    q_hid_batch_test, answer_batch_test, answer_atn_batch_test = q_hid_batch_test.to(device), answer_batch_test.to(device), answer_atn_batch_test.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "      test_output = model(w_embed, q_hid_batch_test, c_hid, c_tensor, c_atn, test=True)\n",
        "    \n",
        "    #use scores as logits\n",
        "    \n",
        "    test_loss = loss_function(answer_batch_test, test_output[2])\n",
        "    return test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3d0jC9j87IQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, test_loader, device, epochs,c_hid, c_tensor, c_atn, w_embed):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  c_hid, c_tensor, c_atn, w_embed = c_hid.to(device), c_tensor.to(device), c_atn.to(device), w_embed.to(device)\n",
        "  optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    total_loss = 0\n",
        "    for i , (q_hid_batch, answer_batch, answer_atn_batch) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      q_hid_batch, answer_batch, answer_atn_batch = q_hid_batch.to(device), answer_batch.to(device), answer_atn_batch.to(device)\n",
        "\n",
        "      logits = model(w_embed, q_hid_batch, c_hid, c_tensor, c_atn, answer_batch, answer_atn_batch)\n",
        "\n",
        "      loss = loss_function(answer_batch, logits)\n",
        "      loss.backward()\n",
        "      total_loss = total_loss + loss\n",
        "     \n",
        "      nn.utils.clip_grad_norm_(model.parameters(),0.5)\n",
        "\n",
        "      optimizer.step()\n",
        "      \n",
        "    val_loss = evaluate(model, test_loader, device, c_hid, c_tensor, c_atn, w_embed)\n",
        "    print(f'Epoch {epoch}, Train_loss: {total_loss/(i+1)}, Val_loss: {val_loss}')\n",
        "     \n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2F6aYY63d30",
        "colab_type": "code",
        "outputId": "5f01f45e-3b51-466e-a93e-f74162dd08b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "model = train(model, train_loader, test_loader, device, epochs,c_hid, c_tensor, c_atn, w_embed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Train_loss: 0.0617070272564888, Val_loss: 1.2739115953445435\n",
            "Epoch 1, Train_loss: 0.0002073720534099266, Val_loss: 1.2754796743392944\n",
            "Epoch 2, Train_loss: 3.130749973934144e-05, Val_loss: 1.274377703666687\n",
            "Epoch 3, Train_loss: 0.00019379214791115373, Val_loss: 1.2688442468643188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWPeuESXXEq3",
        "colab_type": "code",
        "outputId": "84903de1-b9bc-43d1-9436-ce1640e1a793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "q_hid_test.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([136, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp1vE0Qy-uAn",
        "colab_type": "code",
        "outputId": "c6c80b32-db76-4552-ecc8-7321f2798b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        " \n",
        " with torch.no_grad():\n",
        "      check_logits = model(w_embed, q_hid_test[100:102], c_hid, c_tensor, c_atn, test=True)\n",
        " phrase = tokenizer.convert_ids_to_tokens(check_logits[1][0])    \n",
        "#  P.S. sentence has no sence at the moment. what can we do- beam search check,  batch is too small, maybe should use T5 atten, check tokenizer, check pretrained model and maybe use another, better architecture wo concep change - maybe use t5 and reformer ideas"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
